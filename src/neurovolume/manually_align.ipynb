{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "More or less a clone of `bold_register_scratch.ipynb` only now we are going to rely on manually alignment instead of the always failing ML models from ANTs. I don't mean to entirely imply that it is ANTs' fault, I might be misusing the models. However, it's proving difficult, and iterating over something that takes so long to align is taking forever. So lets go with the manual alignment.\n",
    "\n",
    "**steps**\n",
    "We will load all the data, run motion correction, and then manually align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root: /Users/joachimpfefferkorn/repos/neurovolume\n"
     ]
    }
   ],
   "source": [
    "from notebook_viewer_functions import *\n",
    "from functions import *\n",
    "from scivol import *\n",
    "import numpy as np\n",
    "import json\n",
    "import ants\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import pickle\n",
    "import napari\n",
    "import sparse\n",
    "\n",
    "proj_root = parent_directory()\n",
    "print(f\"project root: {proj_root}\")\n",
    "t1_input_filepath = os.path.join(proj_root, \"media/sub-01/anat/sub-01_T1w.nii.gz\")\n",
    "bold_stim_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-emotionalfaces_run-1_bold.nii.gz\")\n",
    "bold_rest_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-rest_bold.nii.gz\")\n",
    "mni_anat_filepath =  os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a.nii\")\n",
    "mni_mask_filepath = os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a_mask.nii\")\n",
    "events_tsv_path = os.path.join(proj_root, \"media/sub-01/func/task-emotionalfaces_run-1_events.tsv\")\n",
    "stimulus_image_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/POFA/fMRI_POFA\"\n",
    "log_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/data/01-subject_emofaces1_2019_Aug_14_1903.log\"\n",
    "cache_folder = \"/Volumes/GlyphA_R1/nvol_cache\"\n",
    "\n",
    "raw_t1_img = ants.image_read(t1_input_filepath)\n",
    "raw_stim_bold = ants.image_read(bold_stim_filepath)\n",
    "raw_rest_bold_img = ants.image_read(bold_rest_filepath)\n",
    "mni_img = ants.image_read(mni_anat_filepath)\n",
    "mni_mask_img = ants.image_read(mni_mask_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_image = ants.image_read(bold_stim_filepath)\n",
    "t1_image = ants.image_read(t1_input_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#replace with napari\n",
    "def compare_bold_alignment_legacy(bold_seq_vol: np.ndarray, anat_vol: np.ndarray, dim='x'):\n",
    "    def x_coord(slice_idx, frame_idx, opacity):\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "        fig.suptitle('x axis view')\n",
    "\n",
    "        axes[0].imshow(bold_seq_vol[slice_idx,:,:, frame_idx], cmap='hot')\n",
    "        axes[0].set_title('BOLD')\n",
    "\n",
    "        axes[1].imshow(anat_vol[slice_idx,:,:], cmap='gray')\n",
    "        axes[1].set_title('Anatomy')\n",
    "\n",
    "        axes[2].imshow(anat_vol[slice_idx,:,:], cmap='gray')\n",
    "        axes[2].imshow(bold_seq_vol[slice_idx,:,:, frame_idx], cmap='hot', alpha=opacity)\n",
    "        axes[2].set_title('Overlay')\n",
    "\n",
    "    def y_coord(slice_idx, frame_idx, opacity):\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "        fig.suptitle('y axis view')\n",
    "\n",
    "        axes[0].imshow(bold_seq_vol[:,slice_idx,:, frame_idx], cmap='hot')\n",
    "        axes[0].set_title('BOLD')\n",
    "\n",
    "        axes[1].imshow(anat_vol[:,slice_idx,:], cmap='gray')\n",
    "        axes[1].set_title('Anatomy')\n",
    "\n",
    "        axes[2].imshow(anat_vol[:,slice_idx,:], cmap='gray')\n",
    "        axes[2].imshow(bold_seq_vol[:,slice_idx,:, frame_idx], cmap='hot', alpha=opacity)\n",
    "        axes[2].set_title('Overlay')\n",
    "\n",
    "    def z_coord(slice_idx, frame_idx, opacity):\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "        fig.suptitle('z axis view')\n",
    "\n",
    "        axes[0].imshow(bold_seq_vol[:,:,slice_idx, frame_idx], cmap='hot')\n",
    "        axes[0].set_title('BOLD')\n",
    "\n",
    "        axes[1].imshow(anat_vol[:,:,slice_idx], cmap='gray')\n",
    "        axes[1].set_title('Anatomy')\n",
    "\n",
    "        axes[2].imshow(anat_vol[:,:,slice_idx], cmap='gray')\n",
    "        axes[2].imshow(bold_seq_vol[:,:,slice_idx, frame_idx], cmap='hot', alpha=opacity)\n",
    "        axes[2].set_title('Overlay')\n",
    "\n",
    "    match dim:\n",
    "        case \"x\":\n",
    "            interact(x_coord, slice_idx=(0, anat_vol.shape[0]-1), frame_idx=(0, bold_seq_vol.shape[3]-1),opacity=(0, 1.0))\n",
    "        case 'y':\n",
    "            interact(y_coord, slice_idx=(0, anat_vol.shape[1]-1), frame_idx=(0, bold_seq_vol.shape[3]-1),opacity=(0, 1.0))\n",
    "        case 'z':\n",
    "            interact(z_coord, slice_idx=(0, anat_vol.shape[2]-1), frame_idx=(0, bold_seq_vol.shape[3]-1),opacity=(0, 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Meat of it All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated Version\n",
    "sliced = bold_image.numpy()[:, :, :, :10]\n",
    "bold_truncated_img = ants.from_numpy(sliced, spacing=bold_image.spacing, origin=bold_image.origin, direction=bold_image.direction)\n",
    "#stabilized = ants.motion_correction(bold_image)\n",
    "stabilized_truncated = ants.motion_correction(bold_truncated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 35, 10)\n",
      "float32\n",
      "float32\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(bold_truncated_img.numpy().shape)\n",
    "print(bold_truncated_img.numpy().dtype)\n",
    "print(t1_image.numpy().dtype)\n",
    "print(t1_image.numpy().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_stabilized_bold_to_anat_p1(bold_img, t1_img, template_frame_idx=0, cache_dir=cache_folder):\n",
    "    \"\"\"\"\n",
    "    This function aligns a 4D BOLD image to a T1 anatomy image\n",
    "    by aligning the mean of the BOLD to the T1 anatomy.\n",
    "    It uses frame registration from only one \"template frame\"\n",
    "    as we are assuming you're using a motion corrected\n",
    "    BOLD image (or something relatively stable) as your input\n",
    "    \n",
    "    It gets things in the ballpark, but does will require some\n",
    "    extra manual registration afterwards.\n",
    "    \"\"\"\n",
    "\n",
    "    #This should also work with a mean image, I wonder what's better? TODO figure out this question\n",
    "    print(\"Creating template frame\")\n",
    "    template_frame_idx = ants.from_numpy(bold_img.numpy()[:,:,:,template_frame_idx], spacing=bold_img.spacing[:3])\n",
    "    frame_registration = ants.registration(\n",
    "        fixed=t1_img,\n",
    "        moving=template_frame_idx,\n",
    "        type_of_transform=\"Rigid\", \n",
    "    )\n",
    "    output_shape = (t1_image.numpy().shape[0], t1_image.numpy().shape[1], t1_image.numpy().shape[2], bold_img.numpy().shape[3])\n",
    "    registered_frames = np.zeros(shape=output_shape, dtype=bold_image.dtype)\n",
    "\n",
    "    for frame in range(bold_img.shape[3]):\n",
    "        print(f\"aligning frame {frame + 1}/{bold_img.shape[3]} \")\n",
    "        bold_frame = ants.from_numpy(bold_img.numpy()[:,:,:,frame],\n",
    "                                    spacing=bold_img.spacing[:3])\n",
    "        registered_frame_data = sparse.COO.from_numpy(ants.apply_transforms(\n",
    "            fixed=t1_img,\n",
    "            moving=bold_frame,\n",
    "            transformlist=frame_registration['fwdtransforms'],\n",
    "            interpolator='linear'\n",
    "        ).numpy())\n",
    "        registered_frames[:,:,:,frame] = registered_frame_data\n",
    "    #sparse_registered_frames = sparse.COO.from_numpy(registered_frames) #Honestly we might not even need sparse and it crashed right at this moment\n",
    "    #Lets just use the numpy for now and assess the memory load\n",
    "    print(\"done\")\n",
    "    return registered_frames\n",
    "\n",
    "#breaking this up into multiple functions for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering just the truncated bold back to ANTS was taking upwards of 20 minutes. Upon reflection, I began to wonder if it was even necessary? We're casting this to numpy arrays anyways, why go back to ANTS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating template frame\n",
      "aligning frame 1/10 \n",
      "aligning frame 2/10 \n",
      "aligning frame 3/10 \n",
      "aligning frame 4/10 \n",
      "aligning frame 5/10 \n",
      "aligning frame 6/10 \n",
      "aligning frame 7/10 \n",
      "aligning frame 8/10 \n",
      "aligning frame 9/10 \n",
      "aligning frame 10/10 \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "registered_BOLD = align_stabilized_bold_to_anat_p1(stabilized_truncated['motion_corrected'], t1_image) #actually not sparse, whoops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_registered_bold = np.transpose(registered_BOLD, axes=(3,0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 296, 10)\n",
      "(10, 512, 512, 296)\n"
     ]
    }
   ],
   "source": [
    "print(registered_BOLD.shape)\n",
    "print(transposed_registered_bold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 17:51:09.449 Python[84381:3127298] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'Image' at 0x37613dad0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "File \u001b[0;32m~/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/_qt/threads/status_checker.py:100\u001b[0m, in \u001b[0;36mStatusChecker.calculate_status\u001b[0;34m(self=<napari._qt.threads.status_checker.StatusChecker object>)\u001b[0m\n",
      "\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 100\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calc_status_from_cursor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "        viewer \u001b[0;34m= Viewer(camera=Camera(center=(255.5, 255.5, 147.5), zoom=np.float64(1.5938476562499997), angles=(np.float64(80.68049771468225), np.float64(0.04245501672671794), np.float64(119.42796003915873)), perspective=0.0, mouse_pan=True, mouse_zoom=True), cursor=Cursor(position=(np.float64(5.0), np.float64(256.3866496405162), np.float64(256.8675941035636), np.float64(282.8754390206965)), scaled=True, style=<CursorStyle.STANDARD: 'standard'>, size=1.0), dims=Dims(ndim=4, ndisplay=3, order=(0, 1, 2, 3), axis_labels=('0', '1', '2', '3'), rollable=(True, True, True, True), range=(RangeTuple(start=np.float64(0.0), stop=np.float64(9.0), step=np.float64(1.0)), RangeTuple(start=np.float64(0.0), stop=np.float64(511.0), step=np.float64(1.0)), RangeTuple(start=np.float64(0.0), stop=np.float64(511.0), step=np.float64(1.0)), RangeTuple(start=np.float64(0.0), stop=np.float64(295.0), step=np.float64(1.0))), margin_left=(0.0, 0.0, 0.0, 0.0), margin_right=(0.0, 0.0, 0.0, 0.0), point=(np.float64(5.0), np.float64(255.0), np.float64(255.0), np.float64(147.0)), last_used=0), grid=GridCanvas(stride=1, shape=(-1, -1), enabled=False), layers=[<Image layer 'Image' at 0x37613dad0>, <Image layer 'transposed_registered_bold' at 0x3b69cef50>], help='use <2> for transform', status={'layer_name': 'transposed_registered_bold', 'layer_base': 'transposed_registered_bold', 'source_type': '', 'plugin': '', 'coordinates': ' [5 279 298 301]'}, tooltip=Tooltip(visible=False, text=''), theme='dark', title='napari', mouse_over_canvas=True, mouse_move_callbacks=[], mouse_drag_callbacks=[], mouse_double_click_callbacks=[], mouse_wheel_callbacks=[<function dims_scroll at 0x345293100>], _persisted_mouse_event={}, _mouse_drag_gen={}, _mouse_wheel_gen={}, _keymap={})\u001b[0m\n",
      "\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover # noqa: BLE001\u001b[39;00m\n",
      "\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Our codebase is not threadsafe. It is possible that an\u001b[39;00m\n",
      "\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# ViewerModel or Layer state is changed while we are trying to\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# We do not want to crash the thread to keep the status updates.\u001b[39;00m\n",
      "\u001b[1;32m    109\u001b[0m     notification_manager\u001b[38;5;241m.\u001b[39mdispatch(Notification\u001b[38;5;241m.\u001b[39mfrom_exception(e))\n",
      "\n",
      "File \u001b[0;32m~/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/components/viewer_model.py:564\u001b[0m, in \u001b[0;36mViewerModel._calc_status_from_cursor\u001b[0;34m(self=Viewer(camera=Camera(center=(255.5, 255.5, 147.5...use_drag_gen={}, _mouse_wheel_gen={}, _keymap={}))\u001b[0m\n",
      "\u001b[1;32m    562\u001b[0m active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mselection\u001b[38;5;241m.\u001b[39mactive\n",
      "\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m active \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 564\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[43mactive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_status\u001b[49m\u001b[43m(\u001b[49m\n",
      "        active \u001b[0;34m= <Image layer 'transposed_registered_bold' at 0x3b69cef50>\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mself \u001b[0;34m= Viewer(camera=Camera(center=(255.5, 255.5, 147.5), zoom=np.float64(1.5938476562499997), angles=(np.float64(80.68049771468225), np.float64(0.04245501672671794), np.float64(119.42796003915873)), perspective=0.0, mouse_pan=True, mouse_zoom=True), cursor=Cursor(position=(np.float64(5.0), np.float64(256.3866496405162), np.float64(256.8675941035636), np.float64(282.8754390206965)), scaled=True, style=<CursorStyle.STANDARD: 'standard'>, size=1.0), dims=Dims(ndim=4, ndisplay=3, order=(0, 1, 2, 3), axis_labels=('0', '1', '2', '3'), rollable=(True, True, True, True), range=(RangeTuple(start=np.float64(0.0), stop=np.float64(9.0), step=np.float64(1.0)), RangeTuple(start=np.float64(0.0), stop=np.float64(511.0), step=np.float64(1.0)), RangeTuple(start=np.float64(0.0), stop=np.float64(511.0), step=np.float64(1.0)), RangeTuple(start=np.float64(0.0), stop=np.float64(295.0), step=np.float64(1.0))), margin_left=(0.0, 0.0, 0.0, 0.0), margin_right=(0.0, 0.0, 0.0, 0.0), point=(np.float64(5.0), np.float64(255.0), np.float64(255.0), np.float64(147.0)), last_used=0), grid=GridCanvas(stride=1, shape=(-1, -1), enabled=False), layers=[<Image layer 'Image' at 0x37613dad0>, <Image layer 'transposed_registered_bold' at 0x3b69cef50>], help='use <2> for transform', status={'layer_name': 'transposed_registered_bold', 'layer_base': 'transposed_registered_bold', 'source_type': '', 'plugin': '', 'coordinates': ' [5 279 298 301]'}, tooltip=Tooltip(visible=False, text=''), theme='dark', title='napari', mouse_over_canvas=True, mouse_move_callbacks=[], mouse_drag_callbacks=[], mouse_double_click_callbacks=[], mouse_wheel_callbacks=[<function dims_scroll at 0x345293100>], _persisted_mouse_event={}, _mouse_drag_gen={}, _mouse_wheel_gen={}, _keymap={})\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mself.cursor.position \u001b[0;34m= (np.float64(5.0), np.float64(256.3866496405162), np.float64(256.8675941035636), np.float64(282.8754390206965))\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mself.cursor \u001b[0;34m= Cursor(position=(np.float64(5.0), np.float64(256.3866496405162), np.float64(256.8675941035636), np.float64(282.8754390206965)), scaled=True, style=<CursorStyle.STANDARD: 'standard'>, size=1.0)\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mself.cursor._view_direction \u001b[0;34m= array([ 0.00000e+00,  8.70974e-01, -4.91329e-01, -7.40980e-04])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mself.dims \u001b[0;34m= Dims(ndim=4, ndisplay=3, order=(0, 1, 2, 3), axis_labels=('0', '1', '2', '3'), rollable=(True, True, True, True), range=(RangeTuple(start=np.float64(0.0), stop=np.float64(9.0), step=np.float64(1.0)), RangeTuple(start=np.float64(0.0), stop=np.float64(511.0), step=np.float64(1.0)), RangeTuple(start=np.float64(0.0), stop=np.float64(511.0), step=np.float64(1.0)), RangeTuple(start=np.float64(0.0), stop=np.float64(295.0), step=np.float64(1.0))), margin_left=(0.0, 0.0, 0.0, 0.0), margin_right=(0.0, 0.0, 0.0, 0.0), point=(np.float64(5.0), np.float64(255.0), np.float64(255.0), np.float64(147.0)), last_used=0)\u001b[0m\n",
      "\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mview_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_view_direction\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdims_displayed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplayed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtooltip\u001b[38;5;241m.\u001b[39mvisible:\n",
      "\u001b[1;32m    572\u001b[0m         tooltip_text \u001b[38;5;241m=\u001b[39m active\u001b[38;5;241m.\u001b[39m_get_tooltip_text(\n",
      "\u001b[1;32m    573\u001b[0m             np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor\u001b[38;5;241m.\u001b[39mposition),\n",
      "\u001b[1;32m    574\u001b[0m             view_direction\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor\u001b[38;5;241m.\u001b[39m_view_direction),\n",
      "\u001b[1;32m    575\u001b[0m             dims_displayed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mdisplayed),\n",
      "\u001b[1;32m    576\u001b[0m             world\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m    577\u001b[0m         )\n",
      "\n",
      "File \u001b[0;32m~/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/layers/base/base.py:2137\u001b[0m, in \u001b[0;36mLayer.get_status\u001b[0;34m(self=<Image layer 'transposed_registered_bold'>, position=array([  5.     , 273.48915, 287.16588, 295.57885]), view_direction=array([ 0.00000e+00,  8.70974e-01, -4.91329e-01, -7.40980e-04]), dims_displayed=[1, 2, 3], world=True)\u001b[0m\n",
      "\u001b[1;32m   2135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   2136\u001b[0m     position \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(position)\n",
      "\u001b[0;32m-> 2137\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\n",
      "        self \u001b[0;34m= <Image layer 'transposed_registered_bold' at 0x3b69cef50>\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mposition \u001b[0;34m= array([  5.     , 273.48915, 287.16588, 295.57885])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mview_direction \u001b[0;34m= array([ 0.00000e+00,  8.70974e-01, -4.91329e-01, -7.40980e-04])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mdims_displayed \u001b[0;34m= [1, 2, 3]\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mworld \u001b[0;34m= True\u001b[0m\n",
      "\u001b[1;32m   2138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mview_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mview_direction\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdims_displayed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims_displayed\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   2144\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/layers/base/base.py:1393\u001b[0m, in \u001b[0;36mLayer.get_value\u001b[0;34m(self=<Image layer 'transposed_registered_bold'>, position=array([  5.     , 273.48915, 287.16588, 295.57885]), view_direction=array([ 0.00000e+00,  8.70974e-01, -4.91329e-01, -7.40980e-04]), dims_displayed=[np.int64(1), np.int64(2), np.int64(3)], world=True)\u001b[0m\n",
      "\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dims_displayed) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[1;32m   1392\u001b[0m         view_direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_world_to_data_ray(view_direction)\n",
      "\u001b[0;32m-> 1393\u001b[0m         start_point, end_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ray_intersections\u001b[49m\u001b[43m(\u001b[49m\n",
      "        self \u001b[0;34m= <Image layer 'transposed_registered_bold' at 0x3b69cef50>\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mposition \u001b[0;34m= array([  5.     , 273.48915, 287.16588, 295.57885])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mview_direction \u001b[0;34m= array([ 0.00000e+00,  8.70974e-01, -4.91329e-01, -7.40980e-04])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mdims_displayed \u001b[0;34m= [np.int64(1), np.int64(2), np.int64(3)]\u001b[0m\n",
      "\u001b[1;32m   1394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mview_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mview_direction\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdims_displayed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims_displayed\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1399\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value_3d(\n",
      "\u001b[1;32m   1400\u001b[0m             start_point\u001b[38;5;241m=\u001b[39mstart_point,\n",
      "\u001b[1;32m   1401\u001b[0m             end_point\u001b[38;5;241m=\u001b[39mend_point,\n",
      "\u001b[1;32m   1402\u001b[0m             dims_displayed\u001b[38;5;241m=\u001b[39mdims_displayed,\n",
      "\u001b[1;32m   1403\u001b[0m         )\n",
      "\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/layers/base/base.py:1869\u001b[0m, in \u001b[0;36mLayer.get_ray_intersections\u001b[0;34m(self=<Image layer 'transposed_registered_bold'>, position=array([  5.     , 273.48915, 287.16588, 295.57885]), view_direction=array([ 0.00000e+00,  8.70974e-01, -4.91329e-01, -7.40980e-04]), dims_displayed=[np.int64(1), np.int64(2), np.int64(3)], world=False)\u001b[0m\n",
      "\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# bounding box is with upper limit excluded in the uses below\u001b[39;00m\n",
      "\u001b[1;32m   1867\u001b[0m bounding_box[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;32m-> 1869\u001b[0m start_point, end_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ray_intersections\u001b[49m\u001b[43m(\u001b[49m\n",
      "        self \u001b[0;34m= <Image layer 'transposed_registered_bold' at 0x3b69cef50>\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mposition \u001b[0;34m= array([  5.     , 273.48915, 287.16588, 295.57885])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mview_direction \u001b[0;34m= array([ 0.00000e+00,  8.70974e-01, -4.91329e-01, -7.40980e-04])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mdims_displayed \u001b[0;34m= [np.int64(1), np.int64(2), np.int64(3)]\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mworld \u001b[0;34m= False\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mbounding_box \u001b[0;34m= array([[  0., 512.],\n",
      "       [  0., 512.],\n",
      "       [  0., 296.]])\u001b[0m\n",
      "\u001b[1;32m   1870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mview_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mview_direction\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdims_displayed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims_displayed\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbounding_box\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounding_box\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_point, end_point\n",
      "\n",
      "File \u001b[0;32m~/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/layers/base/base.py:1949\u001b[0m, in \u001b[0;36mLayer._get_ray_intersections\u001b[0;34m(self=<Image layer 'transposed_registered_bold'>, position=array([  5.5    , 273.98915, 287.66588, 296.07885]), view_direction=array([ 0.00000e+00,  8.70974e-01, -4.91329e-01, -7.40980e-04]), dims_displayed=[np.int64(1), np.int64(2), np.int64(3)], bounding_box=array([[  0., 512.],\n",
      "       [  0., 512.],\n",
      "       [  0., 296.]]), world=False)\u001b[0m\n",
      "\u001b[1;32m   1945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# Calculate ray-bounding box face intersections\u001b[39;00m\n",
      "\u001b[1;32m   1948\u001b[0m start_point_displayed_dimensions \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m-> 1949\u001b[0m     \u001b[43mintersect_line_with_axis_aligned_bounding_box_3d\u001b[49m\u001b[43m(\u001b[49m\n",
      "        view_dir \u001b[0;34m= array([ 8.70974e-01, -4.91329e-01, -7.40980e-04])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mclick_pos_data \u001b[0;34m= array([273.98915, 287.66588, 296.07885])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mfront_face_normal \u001b[0;34m= None\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mbounding_box \u001b[0;34m= array([[  0., 512.],\n",
      "       [  0., 512.],\n",
      "       [  0., 296.]])\u001b[0m\n",
      "\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclick_pos_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounding_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfront_face_normal\u001b[49m\n",
      "\u001b[1;32m   1951\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1952\u001b[0m )\n",
      "\u001b[1;32m   1953\u001b[0m end_point_displayed_dimensions \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m   1954\u001b[0m     intersect_line_with_axis_aligned_bounding_box_3d(\n",
      "\u001b[1;32m   1955\u001b[0m         click_pos_data, view_dir, bounding_box, back_face_normal\n",
      "\u001b[1;32m   1956\u001b[0m     )\n",
      "\u001b[1;32m   1957\u001b[0m )\n",
      "\u001b[1;32m   1959\u001b[0m \u001b[38;5;66;03m# add the coordinates for the axes not displayed\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/utils/geometry.py:739\u001b[0m, in \u001b[0;36mintersect_line_with_axis_aligned_bounding_box_3d\u001b[0;34m(line_point=array([273.98915, 287.66588, 296.07885]), line_direction=array([ 8.70974e-01, -4.91329e-01, -7.40980e-04]), bounding_box=array([[  0., 512.],\n",
      "       [  0., 512.],\n",
      "       [  0., 296.]]), face_normal=None)\u001b[0m\n",
      "\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mintersect_line_with_axis_aligned_bounding_box_3d\u001b[39m(\n",
      "\u001b[1;32m    710\u001b[0m     line_point: np\u001b[38;5;241m.\u001b[39mndarray,\n",
      "\u001b[1;32m    711\u001b[0m     line_direction: np\u001b[38;5;241m.\u001b[39mndarray,\n",
      "\u001b[1;32m    712\u001b[0m     bounding_box: np\u001b[38;5;241m.\u001b[39mndarray,\n",
      "\u001b[1;32m    713\u001b[0m     face_normal: np\u001b[38;5;241m.\u001b[39mndarray,\n",
      "\u001b[1;32m    714\u001b[0m ):\n",
      "\u001b[1;32m    715\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the intersection of a ray with the specified face of an\u001b[39;00m\n",
      "\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    axis-aligned bounding box.\u001b[39;00m\n",
      "\u001b[1;32m    717\u001b[0m \n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m        the specified face.\u001b[39;00m\n",
      "\u001b[1;32m    738\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 739\u001b[0m     front_face_coordinate \u001b[38;5;241m=\u001b[39m \u001b[43mface_coordinate_from_bounding_box\u001b[49m\u001b[43m(\u001b[49m\n",
      "        bounding_box \u001b[0;34m= array([[  0., 512.],\n",
      "       [  0., 512.],\n",
      "       [  0., 296.]])\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mface_normal \u001b[0;34m= None\u001b[0m\n",
      "\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounding_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_normal\u001b[49m\n",
      "\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    742\u001b[0m     intersection_point \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(\n",
      "\u001b[1;32m    743\u001b[0m         intersect_line_with_axis_aligned_plane(\n",
      "\u001b[1;32m    744\u001b[0m             front_face_coordinate,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    748\u001b[0m         )\n",
      "\u001b[1;32m    749\u001b[0m     )\n",
      "\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m intersection_point\n",
      "\n",
      "File \u001b[0;32m~/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/utils/geometry.py:241\u001b[0m, in \u001b[0;36mface_coordinate_from_bounding_box\u001b[0;34m(bounding_box=array([[  0., 512.],\n",
      "       [  0., 512.],\n",
      "       [  0., 296.]]), face_normal=None)\u001b[0m\n",
      "\u001b[1;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the coordinate for a given face in an axis-aligned bounding box.\u001b[39;00m\n",
      "\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03mFor example, if the bounding box has extents [[0, 10], [0, 20], [0, 30]]\u001b[39;00m\n",
      "\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m(ordered zyx), then the face with normal [0, 1, 0] is described by\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    the axis its normal is aligned with.\u001b[39;00m\n",
      "\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    240\u001b[0m axis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere(face_normal)\n",
      "\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mface_normal\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "        axis \u001b[0;34m= array([], shape=(0, 0), dtype=int64)\u001b[0m\u001b[0;34m\n",
      "        \u001b[0mface_normal \u001b[0;34m= None\u001b[0m\n",
      "\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# face is pointing in the positive direction,\u001b[39;00m\n",
      "\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# take the max extent\u001b[39;00m\n",
      "\u001b[1;32m    244\u001b[0m     face_coordinate \u001b[38;5;241m=\u001b[39m bounding_box[axis, \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# face is pointing in the negative direction,\u001b[39;00m\n",
      "\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# take the min extent\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/joachimpfefferkorn/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/_qt/threads/status_checker.py\", line 86, in run\n",
      "    self.calculate_status()\n",
      "  File \"/Users/joachimpfefferkorn/repos/neurovolume/.venv/lib/python3.11/site-packages/napari/_qt/threads/status_checker.py\", line 110, in calculate_status\n",
      "    self.status_and_tooltip_changed.emit(res)\n",
      "                                         ^^^\n",
      "UnboundLocalError: cannot access local variable 'res' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "viewer.add_image(transposed_registered_bold)\n",
    "viewer.add_image(t1_image.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napari can't smoothly handle a truncated `(10, 512, 512, 296)` registered BOLD image, let alone an entire `185` sequence.\n",
    "\n",
    "However, it is proving to be an incredibly valuable tool in checking anatomical alignment.\n",
    "\n",
    "I believe that if I can transform our BOLD matrices in Napari the best workflow is the following:\n",
    "\n",
    "- Skull Strip T1\n",
    "- Skull Strip Bold (if possible/applicable)\n",
    "- Register skull stripped T1 and Bold\n",
    "- Manually align based off a template frame (either the generated mean or a chosen frame) with Napari\n",
    "- Apply alignment to non-skull stripped versions\n",
    "- Add all four grids -skull stripped T1, Full T1, Skull stripped BOLD, Full Bold- to `.nervol`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
