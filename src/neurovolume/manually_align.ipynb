{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "More or less a clone of `bold_register_scratch.ipynb` only now we are going to rely on manually alignment instead of the always failing ML models from ANTs. I don't mean to entirely imply that it is ANTs' fault, I might be misusing the models. However, it's proving difficult, and iterating over something that takes so long to align is taking forever. So lets go with the manual alignment.\n",
    "\n",
    "**steps**\n",
    "We will load all the data, run motion correction, and then manually align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root: /Users/joachimpfefferkorn/repos/neurovolume\n"
     ]
    }
   ],
   "source": [
    "from notebook_viewer_functions import *\n",
    "from functions import *\n",
    "from scivol import *\n",
    "import numpy as np\n",
    "import json\n",
    "import ants\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, Button, Output\n",
    "import pickle\n",
    "import napari\n",
    "import sparse\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "proj_root = parent_directory()\n",
    "print(f\"project root: {proj_root}\")\n",
    "t1_input_filepath = os.path.join(proj_root, \"media/sub-01/anat/sub-01_T1w.nii.gz\")\n",
    "bold_stim_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-emotionalfaces_run-1_bold.nii.gz\")\n",
    "bold_rest_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-rest_bold.nii.gz\")\n",
    "mni_anat_filepath =  os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a.nii\")\n",
    "mni_mask_filepath = os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a_mask.nii\")\n",
    "events_tsv_path = os.path.join(proj_root, \"media/sub-01/func/task-emotionalfaces_run-1_events.tsv\")\n",
    "stimulus_image_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/POFA/fMRI_POFA\"\n",
    "log_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/data/01-subject_emofaces1_2019_Aug_14_1903.log\"\n",
    "cache_folder = \"/Volumes/GlyphA_R1/nvol_cache\"\n",
    "\n",
    "raw_t1_img = ants.image_read(t1_input_filepath)\n",
    "raw_stim_bold = ants.image_read(bold_stim_filepath)\n",
    "raw_rest_bold_img = ants.image_read(bold_rest_filepath)\n",
    "mni_img = ants.image_read(mni_anat_filepath)\n",
    "mni_mask_img = ants.image_read(mni_mask_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_image = ants.image_read(bold_stim_filepath)\n",
    "t1_image = ants.image_read(t1_input_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 5]\n",
      " [0 1 0 6]\n",
      " [0 0 1 7]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#from GPT\n",
    "#uses euler angles\n",
    "translation = np.array([\n",
    "    [1, 0, 0, tx],\n",
    "    [0, 1, 0, ty],\n",
    "    [0, 0, 1, tz],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "scale = np.array([\n",
    "        [sx, 0,  0,  0],\n",
    "        [0,  sy, 0,  0],\n",
    "        [0,  0,  sz, 0],\n",
    "        [0,  0,  0,  1]\n",
    "    ])\n",
    "\n",
    "rotation_x = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, np.cos(rx), -np.sin(rx), 0],\n",
    "    [0, np.sin(rx), np.cos(rx), 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "rotation_y = np.array([\n",
    "    [cos_ry, 0, sin_ry],\n",
    "    [0, 1, 0],\n",
    "    [-sin_ry, 0, cos_ry]\n",
    "])\n",
    "\n",
    "rotation_z = np.array([\n",
    "    [cos_rz, -sin_rz, 0],\n",
    "    [sin_rz, cos_rz, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_BOLD(bold_frame: np.ndarray, translation: tuple, scale: tuple, rotation: tuple,):\n",
    "    tx, ty, tz = translation\n",
    "    sx, sy, sz = scale\n",
    "    rx, ry, rz = rotation\n",
    "    translation = np.array([\n",
    "        [1, 0, 0, tx],\n",
    "        [0, 1, 0, ty],\n",
    "        [0, 0, 1, tz],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "# Translation matrices copypasta from GPT\n",
    "    scale = np.array([\n",
    "            [sx, 0,  0,  0],\n",
    "            [0,  sy, 0,  0],\n",
    "            [0,  0,  sz, 0],\n",
    "            [0,  0,  0,  1]\n",
    "        ])\n",
    "\n",
    "# I wonder where the origin is here...\n",
    "    rotation_x = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, np.cos(rx), -np.sin(rx), 0],\n",
    "        [0, np.sin(rx), np.cos(rx), 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    rotation_y = np.array([\n",
    "        [np.cos(ry), 0, np.sin(ry), 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-np.sin(ry), 0, np.cos(ry), 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    rotation_z = np.array([\n",
    "        [np.cos(rz), -np.sin(rz), 0, 0],\n",
    "        [np.sin(rz), np.cos(rz), 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    rotation_matrix = rotation_x @ rotation_y @ rotation_z\n",
    "    transformation_matrix = translation @ rotation @ scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO try torch sparse?\n",
    "\n",
    "\n",
    "\n",
    "def manual_bold_alignment(bold_seq_vol: np.ndarray, anat_vol: np.ndarray,\n",
    "                                  dim='x', bold_cmap = 'viridis', anat_cmap = 'gray'):\n",
    "    def x_coord(slice_idx, frame_idx, opacity, rotation):\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "        fig.suptitle('x axis view')\n",
    "\n",
    "        bold_slice = bold_seq_vol[slice_idx,:,:, frame_idx]\n",
    "\n",
    "        bold_im = axes[0].imshow(bold_slice, cmap=bold_cmap)\n",
    "        bold_im.set_transform(mtransforms.Affine2D().rotate_deg(rotation))\n",
    "        axes[0].set_title('BOLD')\n",
    "\n",
    "        axes[1].imshow(anat_vol[slice_idx,:,:], cmap=anat_cmap)\n",
    "        axes[1].set_title('Anatomy')\n",
    "\n",
    "        axes[2].imshow(anat_vol[slice_idx,:,:], cmap=anat_cmap)\n",
    "        axes[2].imshow(bold_slice, cmap=bold_cmap, alpha=opacity)\n",
    "        axes[2].set_title('Overlay')\n",
    "\n",
    "        return rotation\n",
    "    \n",
    "    \n",
    "    def y_coord(slice_idx, frame_idx, opacity):\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "        fig.suptitle('y axis view')\n",
    "\n",
    "        axes[0].imshow(bold_seq_vol[:,slice_idx,:, frame_idx], cmap=bold_cmap)\n",
    "        axes[0].set_title('BOLD')\n",
    "\n",
    "        axes[1].imshow(anat_vol[:,slice_idx,:], cmap=anat_cmap)\n",
    "        axes[1].set_title('Anatomy')\n",
    "\n",
    "        axes[2].imshow(anat_vol[:,slice_idx,:], cmap=anat_cmap)\n",
    "        axes[2].imshow(bold_seq_vol[:,slice_idx,:, frame_idx], cmap=bold_cmap, alpha=opacity)\n",
    "        axes[2].set_title('Overlay')\n",
    "\n",
    "    def z_coord(slice_idx, frame_idx, opacity):\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "        fig.suptitle('z axis view')\n",
    "\n",
    "        axes[0].imshow(bold_seq_vol[:,:,slice_idx, frame_idx], cmap=bold_cmap)\n",
    "        axes[0].set_title('BOLD')\n",
    "\n",
    "        axes[1].imshow(anat_vol[:,:,slice_idx], cmap=anat_cmap)\n",
    "        axes[1].set_title('Anatomy')\n",
    "\n",
    "        axes[2].imshow(anat_vol[:,:,slice_idx], cmap=anat_cmap)\n",
    "        axes[2].imshow(bold_seq_vol[:,:,slice_idx, frame_idx], cmap=bold_cmap, alpha=opacity)\n",
    "        axes[2].set_title('Overlay')\n",
    "\n",
    "    match dim:\n",
    "        case \"x\":\n",
    "            interact(x_coord, slice_idx=(0, anat_vol.shape[0]-1), frame_idx=(0, bold_seq_vol.shape[3]-1),opacity=(0, 1.0), rotation=(0,365))\n",
    "        case 'y':\n",
    "            interact(y_coord, slice_idx=(0, anat_vol.shape[1]-1), frame_idx=(0, bold_seq_vol.shape[3]-1),opacity=(0, 1.0))\n",
    "        case 'z':\n",
    "            interact(z_coord, slice_idx=(0, anat_vol.shape[2]-1), frame_idx=(0, bold_seq_vol.shape[3]-1),opacity=(0, 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Meat of it All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated Version\n",
    "sliced = bold_image.numpy()[:, :, :, :10]\n",
    "bold_truncated_img = ants.from_numpy(sliced, spacing=bold_image.spacing, origin=bold_image.origin, direction=bold_image.direction)\n",
    "#stabilized = ants.motion_correction(bold_image)\n",
    "stabilized_truncated = ants.motion_correction(bold_truncated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bold_truncated_img.numpy().shape)\n",
    "print(bold_truncated_img.numpy().dtype)\n",
    "print(t1_image.numpy().dtype)\n",
    "print(t1_image.numpy().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_stabilized_bold_to_anat_p1(bold_img, t1_img, template_frame_idx=0, cache_dir=cache_folder):\n",
    "    \"\"\"\"\n",
    "    This function aligns a 4D BOLD image to a T1 anatomy image\n",
    "    by aligning the mean of the BOLD to the T1 anatomy.\n",
    "    It uses frame registration from only one \"template frame\"\n",
    "    as we are assuming you're using a motion corrected\n",
    "    BOLD image (or something relatively stable) as your input\n",
    "    \n",
    "    It gets things in the ballpark, but does will require some\n",
    "    extra manual registration afterwards.\n",
    "    \"\"\"\n",
    "\n",
    "    #This should also work with a mean image, I wonder what's better? TODO figure out this question\n",
    "    print(\"Creating template frame\")\n",
    "    template_frame_idx = ants.from_numpy(bold_img.numpy()[:,:,:,template_frame_idx], spacing=bold_img.spacing[:3])\n",
    "    frame_registration = ants.registration(\n",
    "        fixed=t1_img,\n",
    "        moving=template_frame_idx,\n",
    "        type_of_transform=\"Rigid\", \n",
    "    )\n",
    "    output_shape = (t1_image.numpy().shape[0], t1_image.numpy().shape[1], t1_image.numpy().shape[2], bold_img.numpy().shape[3])\n",
    "    registered_frames = np.zeros(shape=output_shape, dtype=bold_image.dtype)\n",
    "\n",
    "    for frame in range(bold_img.shape[3]):\n",
    "        print(f\"aligning frame {frame + 1}/{bold_img.shape[3]} \")\n",
    "        bold_frame = ants.from_numpy(bold_img.numpy()[:,:,:,frame],\n",
    "                                    spacing=bold_img.spacing[:3])\n",
    "        registered_frame_data = sparse.COO.from_numpy(ants.apply_transforms(\n",
    "            fixed=t1_img,\n",
    "            moving=bold_frame,\n",
    "            transformlist=frame_registration['fwdtransforms'],\n",
    "            interpolator='linear'\n",
    "        ).numpy())\n",
    "        registered_frames[:,:,:,frame] = registered_frame_data\n",
    "    #sparse_registered_frames = sparse.COO.from_numpy(registered_frames) #Honestly we might not even need sparse and it crashed right at this moment\n",
    "    #Lets just use the numpy for now and assess the memory load\n",
    "    print(\"done\")\n",
    "    return registered_frames\n",
    "\n",
    "#breaking this up into multiple functions for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering just the truncated bold back to ANTS was taking upwards of 20 minutes. Upon reflection, I began to wonder if it was even necessary? We're casting this to numpy arrays anyways, why go back to ANTS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating template frame\n",
      "aligning frame 1/10 \n",
      "aligning frame 2/10 \n",
      "aligning frame 3/10 \n",
      "aligning frame 4/10 \n",
      "aligning frame 5/10 \n",
      "aligning frame 6/10 \n",
      "aligning frame 7/10 \n",
      "aligning frame 8/10 \n",
      "aligning frame 9/10 \n",
      "aligning frame 10/10 \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "registered_BOLD = align_stabilized_bold_to_anat_p1(stabilized_truncated['motion_corrected'], t1_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_bold_alignment(registered_BOLD, t1_image.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napari can't smoothly handle a truncated `(10, 512, 512, 296)` registered BOLD image, let alone an entire `185` sequence.\n",
    "\n",
    "However, it is proving to be an incredibly valuable tool in checking anatomical alignment.\n",
    "\n",
    "I believe that if I can transform our BOLD matrices in Napari the best workflow is the following:\n",
    "\n",
    "**Actually let's just do the alignment in matplot lib and then verify it with Napari**\n",
    "\n",
    "- Skull Strip T1\n",
    "- Skull Strip Bold (if possible/applicable)\n",
    "- Register skull stripped T1 and Bold\n",
    "- Manually align based off a template frame (either the generated mean or a chosen frame) with Napari\n",
    "- Apply alignment to non-skull stripped versions\n",
    "- Add all four grids -skull stripped T1, Full T1, Skull stripped BOLD, Full Bold- to `.nervol`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
