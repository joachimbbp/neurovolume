{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "More or less a clone of `bold_register_scratch.ipynb` only now we are going to rely on manually alignment instead of the always failing ML models from ANTs. I don't mean to entirely imply that it is ANTs' fault, I might be misusing the models. However, it's proving difficult, and iterating over something that takes so long to align is taking forever. So lets go with the manual alignment.\n",
    "\n",
    "**steps**\n",
    "We will load all the data, run motion correction, and then manually align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root: /Users/joachimpfefferkorn/repos/neurovolume\n"
     ]
    }
   ],
   "source": [
    "from notebook_viewer_functions import *\n",
    "from functions import *\n",
    "from scivol import *\n",
    "import numpy as np\n",
    "import json\n",
    "import ants\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "proj_root = parent_directory()\n",
    "print(f\"project root: {proj_root}\")\n",
    "t1_input_filepath = os.path.join(proj_root, \"media/sub-01/anat/sub-01_T1w.nii.gz\")\n",
    "bold_stim_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-emotionalfaces_run-1_bold.nii.gz\")\n",
    "bold_rest_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-rest_bold.nii.gz\")\n",
    "mni_anat_filepath =  os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a.nii\")\n",
    "mni_mask_filepath = os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a_mask.nii\")\n",
    "events_tsv_path = os.path.join(proj_root, \"media/sub-01/func/task-emotionalfaces_run-1_events.tsv\")\n",
    "stimulus_image_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/POFA/fMRI_POFA\"\n",
    "log_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/data/01-subject_emofaces1_2019_Aug_14_1903.log\"\n",
    "\n",
    "raw_t1_img = ants.image_read(t1_input_filepath)\n",
    "raw_stim_bold = ants.image_read(bold_stim_filepath)\n",
    "raw_rest_bold_img = ants.image_read(bold_rest_filepath)\n",
    "mni_img = ants.image_read(mni_anat_filepath)\n",
    "mni_mask_img = ants.image_read(mni_mask_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_image = ants.image_read(bold_stim_filepath)\n",
    "t1_image = ants.image_read(t1_input_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copypasta from `bold_restister_scratch.ipynb`\n",
    "\n",
    "def explore_fMRI(ants_img: ants.core.ants_image.ANTsImage,\n",
    "                volume_override = \"NULL\",\n",
    "                 dim=\"x\", events_tsv=\"NULL\",\n",
    "                 cmap='nipy_spectral'):\n",
    "    if type(volume_override) == np.ndarray:\n",
    "        vol = volume_override\n",
    "    else:\n",
    "        vol = ants_img.numpy()\n",
    "    \n",
    "    def dim_to_indexed(dim, slice, frame):\n",
    "        match dim:\n",
    "            case \"x\":\n",
    "                return vol[slice,:,:,frame]\n",
    "            case \"y\":\n",
    "                return vol[:,slice,:,frame]\n",
    "            case \"z\":\n",
    "                return vol[:,:,slice,frame]\n",
    "\n",
    "    def plot(slice, frame):\n",
    "        second = float(frame * ants_img.spacing[3])\n",
    "        plt.figure()\n",
    "        plt.imshow(dim_to_indexed(dim, slice, frame), cmap=cmap)\n",
    "        plt.show()\n",
    "        present_event = \"No event file\"\n",
    "        if events_tsv != \"NULL\":\n",
    "            for event in events_tsv.split(\"\\n\"):\n",
    "                info = event.split(\"\t\")\n",
    "                if info[0].isdigit() and info[1].isdigit():\n",
    "                    if float(info[0]) <= second < float(info[0] + info[1]):\n",
    "                        present_event = info[2]\n",
    "            print(present_event)\n",
    "\n",
    "    frame_slider = (0, (vol.shape[3]-1))\n",
    "    match dim:\n",
    "        case \"x\":\n",
    "            interact(plot, slice=(0, vol.shape[0]-1), frame=frame_slider)\n",
    "        case \"y\":\n",
    "            interact(plot, slice=(0, vol.shape[1]-1), frame=frame_slider)\n",
    "        case \"z\":\n",
    "            interact(plot, slice=(0, vol.shape[2]-1), frame=frame_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bold_alignment(bold_seq_vol: np.ndarray, anat_vol: np.ndarray, dim='x'):\n",
    "    def x_coord(slice_idx, frame_idx, opacity):\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "        fig.suptitle('x axis view')\n",
    "\n",
    "        axes[0].imshow(bold_seq_vol[slice_idx,:,:, frame_idx], cmap='hot')\n",
    "        axes[0].set_title('BOLD')\n",
    "\n",
    "        axes[1].imshow(anat_vol[slice_idx,:,:], cmap='gray')\n",
    "        axes[1].set_title('Anatomy')\n",
    "\n",
    "        axes[2].imshow(anat_vol[slice_idx,:,:], cmap='gray')\n",
    "        axes[2].imshow(bold_seq_vol[slice_idx,:,:, frame_idx], cmap='hot', alpha=opacity)\n",
    "        axes[2].set_title('Overlay')\n",
    "\n",
    "    def y_coord(slice_idx, frame_idx, opacity):\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "        fig.suptitle('y axis view')\n",
    "\n",
    "        axes[0].imshow(bold_seq_vol[:,slice_idx,:, frame_idx], cmap='hot')\n",
    "        axes[0].set_title('BOLD')\n",
    "\n",
    "        axes[1].imshow(anat_vol[:,slice_idx,:], cmap='gray')\n",
    "        axes[1].set_title('Anatomy')\n",
    "\n",
    "        axes[2].imshow(anat_vol[:,slice_idx,:], cmap='gray')\n",
    "        axes[2].imshow(bold_seq_vol[:,slice_idx,:, frame_idx], cmap='hot', alpha=opacity)\n",
    "        axes[2].set_title('Overlay')\n",
    "\n",
    "    def z_coord(slice_idx, frame_idx, opacity):\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "        fig.suptitle('z axis view')\n",
    "\n",
    "        axes[0].imshow(bold_seq_vol[:,:,slice_idx, frame_idx], cmap='hot')\n",
    "        axes[0].set_title('BOLD')\n",
    "\n",
    "        axes[1].imshow(anat_vol[:,:,slice_idx], cmap='gray')\n",
    "        axes[1].set_title('Anatomy')\n",
    "\n",
    "        axes[2].imshow(anat_vol[:,:,slice_idx], cmap='gray')\n",
    "        axes[2].imshow(bold_seq_vol[:,:,slice_idx, frame_idx], cmap='hot', alpha=opacity)\n",
    "        axes[2].set_title('Overlay')\n",
    "\n",
    "    match dim:\n",
    "        case \"x\":\n",
    "            interact(x_coord, slice_idx=(0, anat_vol.shape[0]-1), frame_idx=(0, bold_seq_vol.shape[3]-1),opacity=(0, 1.0))\n",
    "        case 'y':\n",
    "            interact(y_coord, slice_idx=(0, anat_vol.shape[1]-1), frame_idx=(0, bold_seq_vol.shape[3]-1),opacity=(0, 1.0))\n",
    "        case 'z':\n",
    "            interact(z_coord, slice_idx=(0, anat_vol.shape[2]-1), frame_idx=(0, bold_seq_vol.shape[3]-1),opacity=(0, 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Meat of it All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently not using this:\n",
    "sliced = bold_image.numpy()[:, :, :, :3]\n",
    "bold_truncated_img = ants.from_numpy(sliced, spacing=bold_image.spacing, origin=bold_image.origin, direction=bold_image.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stabilized_truncated = ants.motion_correction(bold_truncated_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! The `compare_bold_alignment()` not work because the bold image is not registered to the t1 image. That is to say; it is not in the correct resolution. Let's do a minimal registration where we just blow it up to get the registrations the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_stabilized_bold_to_anat(bold_img, t1_img, template_frame_idx=0):\n",
    "    \"\"\"\"\n",
    "    This function aligns a 4D BOLD image to a T1 anatomy image\n",
    "    by aligning the mean of the BOLD to the T1 anatomy.\n",
    "    It uses frame registration from only one \"template frame\"\n",
    "    as we are assuming you're using a motion corrected\n",
    "    BOLD image (or something relatively stable) as your input\n",
    "    \n",
    "    It gets things in the ballpark, but does will require some\n",
    "    extra manual registration afterwards.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Aligning stabilized bold to anat\\nEstablishing temporal mean\")\n",
    "    template_frame_idx = ants.from_numpy(bold_img.numpy()[:,:,:,template_frame_idx], spacing=bold_img.spacing[:3])\n",
    "\n",
    "    frame_registration = ants.registration(\n",
    "        fixed=t1_img,\n",
    "        moving=template_frame_idx,\n",
    "        type_of_transform=\"Rigid\", \n",
    "    )\n",
    "    registered_frames = []\n",
    "\n",
    "    print(\"Applying transformations to frames\")\n",
    "    for frame in range(bold_img.shape[3]):\n",
    "        print(f\"frame{frame}/{bold_img.shape[3]}\")\n",
    "        print(\"creating image of bold frame\")\n",
    "        bold_frame = ants.from_numpy(bold_img.numpy()[:,:,:,frame],\n",
    "                                     spacing=bold_img.spacing[:3])\n",
    "        print(\"     Applying frame transformation\")\n",
    "        registered_frame = ants.apply_transforms(\n",
    "            fixed=t1_img,\n",
    "            moving=bold_frame,\n",
    "            transformlist=frame_registration['fwdtransforms'],\n",
    "            interpolator='linear'\n",
    "        )\n",
    "        print(\"     adding registered frame to list\")\n",
    "        registered_frames.append(registered_frame)\n",
    "    print(\"Creating 4D numpy vol from list of 3D ANTs imgs\")\n",
    "    data = np.stack([frame.numpy() for frame in registered_frames], axis=3)\n",
    "    print(\"Creating 4D bold image from numpy vol\")\n",
    "    registered_bold_img = ants.from_numpy(data, origin=bold_img.origin, spacing=bold_img.spacing)\n",
    "    return registered_bold_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just going with the truncated for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning stabilized bold to anat\n",
      "Establishing temporal mean\n",
      "Applying transformations to frames\n",
      "frame0/3\n",
      "creating image of bold frame\n",
      "     Applying frame transformation\n",
      "     adding registered frame to list\n",
      "frame1/3\n",
      "creating image of bold frame\n",
      "     Applying frame transformation\n",
      "     adding registered frame to list\n",
      "frame2/3\n",
      "creating image of bold frame\n",
      "     Applying frame transformation\n",
      "     adding registered frame to list\n",
      "Creating 4D numpy vol from list of 3D ANTs imgs\n",
      "Creating 4D bold image from numpy vol\n"
     ]
    }
   ],
   "source": [
    "registered_seq = align_stabilized_bold_to_anat(stabilized_truncated['motion_corrected'], t1_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94e0d7e2c9f479e97e1e0f1c2f9321b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=147, description='slice_idx', max=295), IntSlider(value=1, description='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_bold_alignment(registered_seq.numpy(), t1_image.numpy(), dim=\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this works, we'll write our manual alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
