{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering fMRI Data\n",
    "Both anatomical and functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_viewer_functions import *\n",
    "from functions import *\n",
    "from scivol import *\n",
    "import numpy as np\n",
    "import json\n",
    "import ants\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root: /Users/joachimpfefferkorn/repos/neurovolume\n"
     ]
    }
   ],
   "source": [
    "proj_root = parent_directory()\n",
    "print(f\"project root: {proj_root}\")\n",
    "t1_input_filepath = os.path.join(proj_root, \"media/sub-01/anat/sub-01_T1w.nii.gz\")\n",
    "bold_stim_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-emotionalfaces_run-1_bold.nii.gz\")\n",
    "bold_rest_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-rest_bold.nii.gz\")\n",
    "mni_anat_filepath =  os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a.nii\")\n",
    "mni_mask_filepath = os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a_mask.nii\")\n",
    "events_tsv_path = os.path.join(proj_root, \"media/sub-01/func/task-emotionalfaces_run-1_events.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_t1_img = ants.image_read(t1_input_filepath)\n",
    "raw_stim_bold = ants.image_read(bold_stim_filepath)\n",
    "raw_rest_bold_img = ants.image_read(bold_rest_filepath)\n",
    "mni_img = ants.image_read(mni_anat_filepath)\n",
    "mni_mask_img = ants.image_read(mni_mask_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize BOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [study](https://openneuro.org/datasets/ds003548/versions/1.0.1) the resting state fMRI is ten minutes long. Some chatbots suggested that the 4th index in the `ANTsImage` `Spacing` Tuple would be the time spacing, and correspond to the number of seconds each frame is taken at.\n",
    "\n",
    "To verify this, let's make sure that $\\frac{Slice Duration * Number Of Slices}{60}=10$\n",
    "\n",
    "Confusingly enough, Dimensions is the name in the return string when printed, while these values are accessed by `.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTsImage\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (64, 64, 35, 300)\n",
      "\t Spacing    : (4.0, 4.0, 4.0, 2.0)\n",
      "\t Origin     : (-127.953, 108.933, -74.8393, 0.0)\n",
      "\t Direction  : [ 1.  0.  0.  0.  0. -1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.]\n",
      "\n",
      "<class 'int'>\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "print(raw_rest_bold_img)\n",
    "minutes = (raw_rest_bold_img.spacing[3] * float(raw_rest_bold_img.shape[3])) / 60.0\n",
    "print(type(raw_rest_bold_img.shape[3]))\n",
    "print(minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good to me. Let's take this assumption that `.spacing[3]` will be the duration of the slices in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(events_tsv_path, 'r') as f:\n",
    "    events_tsv = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO display actual stimulus\n",
    "# TODO show all three spatial axes in one display\n",
    "# TODO rotate these displays\n",
    "# TODO once integrated add masking\n",
    "\n",
    "def explore_fMRI(ants_img: ants.core.ants_image.ANTsImage,\n",
    "                 dim=\"x\", events_tsv=\"NULL\",\n",
    "                 cmap='nipy_spectral'):\n",
    "    vol = ants_img.numpy()\n",
    "    def dim_to_indexed(dim, slice, frame):\n",
    "        match dim:\n",
    "            case \"x\":\n",
    "                return vol[slice,:,:,frame]\n",
    "            case \"y\":\n",
    "                return vol[:,slice,:,frame]\n",
    "            case \"z\":\n",
    "                return vol[:,:,slice,frame]\n",
    "\n",
    "    def plot(slice, frame):\n",
    "        second = float(frame * ants_img.spacing[3])\n",
    "        plt.figure()\n",
    "        plt.imshow(dim_to_indexed(dim, slice, frame), cmap=cmap)\n",
    "        plt.show()\n",
    "        present_event = \"No event file\"\n",
    "        if events_tsv != \"NULL\":\n",
    "            for event in events_tsv.split(\"\\n\"):\n",
    "                info = event.split(\"\t\")\n",
    "                if info[0].isdigit() and info[1].isdigit():\n",
    "                    if float(info[0]) <= second < float(info[0] + info[1]):\n",
    "                        present_event = info[2]\n",
    "            print(present_event)\n",
    "\n",
    "    frame_slider = (0, (vol.shape[3]-1))\n",
    "    match dim:\n",
    "        case \"x\":\n",
    "            interact(plot, slice=(0, vol.shape[0]-1), frame=frame_slider)\n",
    "        case \"y\":\n",
    "            interact(plot, slice=(0, vol.shape[1]-1), frame=frame_slider)\n",
    "        case \"z\":\n",
    "            interact(plot, slice=(0, vol.shape[2]-1), frame=frame_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc61585de95747af934f68864e01ce9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7e23ceb37e4a36bded774cbc8cd3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cb7fc660314625ada59f5b4f294614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=17, description='slice', max=34), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_fMRI(raw_stim_bold, dim=\"x\", events_tsv=events_tsv)\n",
    "explore_fMRI(raw_stim_bold, dim=\"y\", events_tsv=events_tsv)\n",
    "explore_fMRI(raw_stim_bold, dim=\"z\", events_tsv=events_tsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract Baseline from Stimulus to Isolate the Activations\n",
    "\n",
    "So it looks like this experiment doesn't have a neutral stimulus, just a \"rest\" . This isn't great, I'd much prefer a Block Design. There are a couple of ways we could build an experiment. One thought I had would be to take the mean of the rest state and then read the activations as being against that? *Is this valid?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_average_rest = np.mean(raw_rest_bold_img.numpy(), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_3D_vol(temporal_average_rest)\n",
    "print(temporal_average_rest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_neutral_3D(experimental, neutral):\n",
    "    \"\"\"\n",
    "    Subtracts a 3D neutral from each time slice in a 4D experimental volume\n",
    "    For use in experiments which lack a block design and for which the \n",
    "    neutral stimulus has been derived from an averaged rest state\n",
    "    \"\"\"\n",
    "    result = np.empty_like(experimental)\n",
    "    for time_slice in range(experimental.shape[3]):\n",
    "        result[:,:,:,time_slice] = experimental[:,:,:, time_slice] - neutral\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_BOLD = subtract_neutral_3D(raw_stim_bold.numpy(), temporal_average_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_fMRI(isolated_BOLD, dim=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Change in Volume\n",
    "Here's another possible approach. Again, I do not know the scientific validity of this. Here we'll take each frame and measure the difference in the bold response from the previous frame.\n",
    "\n",
    "Grabbing the absolute value between the current and previous timestamp creates the most convenient isolation for our visualization tool. However, this does not mean that it is the most scientifically valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_BOLD_movement(bold_vol, baseline_vol):\n",
    "    \"\"\"\n",
    "    Each frame shows the difference between it and the previous frame. First frame is initialized at zero. \n",
    "    \"\"\"\n",
    "    result = np.empty_like(bold_vol)\n",
    "    for time_slice in range(1, bold_vol.shape[3]):\n",
    "        result[:,:,:,time_slice] = np.absolute(bold_vol[:,:,:,time_slice] - bold_vol[:,:,:,time_slice - 1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_movement = measure_BOLD_movement(raw_stim_bold.numpy(), temporal_average_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_fMRI(bold_movement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register and Mask BOLD and Anat\n",
    "\n",
    "We're going to go wit the `bold_movement` volume. Not sure if it's the best, but it's the most convenient to visualize in this context. Or maybe we mask/register the bold first? Is that the best way of going about it?\n",
    "\n",
    "We're also going to need to account for motion correction and size differences between anat and bold. Oh boy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mni_template = ants.image_read(mni_anat_filepath)\n",
    "mni_mask = ants.image_read(mni_mask_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slicing methodology check\n",
    "sliced_bold = ants.from_numpy(raw_stim_bold.numpy()[:,:,:,50])\n",
    "sliced_bold02 = ants.from_numpy(raw_stim_bold.numpy()[:,:,:,100])\n",
    "explore_3D_vol(sliced_bold.numpy())\n",
    "print(sliced_bold is sliced_bold02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_masking(bold_img, template, mask, dilate=True):\n",
    "    indent = \"        ➡️\"\n",
    "    print(\"Masking bold. This might take a while...\")\n",
    "    result = np.empty_like(bold_img.numpy())\n",
    "\n",
    "\n",
    "    for time_slice in range(bold_img.numpy().shape[3]):\n",
    "        bold_slice = ants.from_numpy(bold_img.numpy()[:,:,:,time_slice])\n",
    "\n",
    "        print(f\"Masking for time slice {time_slice} out of {bold_img.numpy().shape[3]}\")\n",
    "        print(f\"{indent}creating template\")\n",
    "        template_warp_to_bold_anat = ants.registration(\n",
    "            fixed=bold_slice,\n",
    "            moving=template, \n",
    "            type_of_transform='SyN',\n",
    "            verbose=False\n",
    "            )\n",
    "        \n",
    "        print(f\"{indent}Registering template image\")\n",
    "\n",
    "        print(f\"{indent}Creating brain mask\")\n",
    "        brain_mask = ants.apply_transforms(\n",
    "            fixed=template_warp_to_bold_anat['warpedmovout'],\n",
    "            moving=mask,\n",
    "            transformlist=template_warp_to_bold_anat['fwdtransforms'],\n",
    "            interpolator='nearestNeighbor',\n",
    "            verbose=False\n",
    "            )\n",
    "        if dilate:\n",
    "            print(f\"{indent}Dilating brian mask\")\n",
    "            brain_mask = ants.morphology(brain_mask, radius=4, operation='dilate', mtype='binary')\n",
    "        print(f\"{indent}Applying brain mask and adding to final result\")\n",
    "        result[:,:,:,time_slice] = ants.mask_image(bold_slice, brain_mask).numpy()\n",
    "    return ants.from_numpy(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masked_bold = bold_masking(raw_stim_bold, mni_template, mni_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore_4D_vol(masked_bold)\n",
    "# explore_4D_vol(raw_stim_bold.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it looks like this is the exact same thing\n",
    "\n",
    "looking at the mri, though, do we even need to mask the brain? Can we just threshold out the purple stuff?\n",
    "\n",
    "The following is a very lovely function but it wasn't very smart of you to write it. If you look at the BOLD respoonse in this dataset you can see that we can isolate the brain just with grid-specific threshholding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register BOLD to T1\n",
    "\n",
    "Like the above function, this will register the BOLD function to the anat. A more typical analysis pipeline might register to MNI space, as above, but we don't really care about voxel-specific cross-study analysis; we want a subject-anatomy specific visualization.\n",
    "\n",
    "We're reusing much of the logic above, and will have to write custom viewer functions to verify our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = ants.image_read(\"/Users/joachimpfefferkorn/Downloads/sub-01122021301_task-arousal_bold.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_fMRI(new_dataset.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_dataset)\n",
    "print(raw_stim_bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
