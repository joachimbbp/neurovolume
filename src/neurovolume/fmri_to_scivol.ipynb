{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering fMRI Data\n",
    "Both anatomical and functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_viewer_functions import *\n",
    "from functions import *\n",
    "from scivol import *\n",
    "import numpy as np\n",
    "import json\n",
    "import ants\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root: /Users/joachimpfefferkorn/repos/neurovolume\n"
     ]
    }
   ],
   "source": [
    "proj_root = parent_directory()\n",
    "print(f\"project root: {proj_root}\")\n",
    "t1_input_filepath = os.path.join(proj_root, \"media/sub-01/anat/sub-01_T1w.nii.gz\")\n",
    "bold_stim_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-emotionalfaces_run-1_bold.nii.gz\")\n",
    "bold_rest_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-rest_bold.nii.gz\")\n",
    "mni_anat_filepath =  os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a.nii\")\n",
    "mni_mask_filepath = os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a_mask.nii\")\n",
    "events_tsv_path = os.path.join(proj_root, \"media/sub-01/func/task-emotionalfaces_run-1_events.tsv\")\n",
    "stimulus_image_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/POFA/fMRI_POFA\"\n",
    "log_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/data/01-subject_emofaces1_2019_Aug_14_1903.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given how specific this notebook is getting to this study, maybe rename the stimulus files to something like `emofaces_run_1_bold`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_t1_img = ants.image_read(t1_input_filepath)\n",
    "raw_stim_bold = ants.image_read(bold_stim_filepath)\n",
    "raw_rest_bold_img = ants.image_read(bold_rest_filepath)\n",
    "mni_img = ants.image_read(mni_anat_filepath)\n",
    "mni_mask_img = ants.image_read(mni_mask_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Stimulus Data\n",
    "I am somewhat mystified by the `csv` formatting for from PsychoPy. I am going to start by parsing it from the log file. This is perhaps not the most robust way to do it, but it will be quicker than reading through and testing all the pyschopy documentation. It also provides a ton of metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181.9064 POFA/fMRI_POFA/021c.tif\n",
      "186.8437 POFA/fMRI_POFA/072c.tif\n",
      "191.8438 POFA/fMRI_POFA/013c.tif\n",
      "196.8440 POFA/fMRI_POFA/033c.tif\n",
      "201.8439 POFA/fMRI_POFA/083c.tif\n",
      "206.8440 POFA/fMRI_POFA/006c.tif\n",
      "241.8758 POFA/fMRI_POFA/036a.tif\n",
      "246.8443 POFA/fMRI_POFA/008a.tif\n",
      "251.8445 POFA/fMRI_POFA/075a.tif\n",
      "256.8601 POFA/fMRI_POFA/049a.tif\n",
      "261.8443 POFA/fMRI_POFA/023a.tif\n",
      "266.8448 POFA/fMRI_POFA/086a.tif\n",
      "271.8603 POFA/fMRI_POFA/069a.tif\n",
      "276.8136 POFA/fMRI_POFA/080a.tif\n",
      "283.4543 POFA/fMRI_POFA/018a.tif\n",
      "287.1106 POFA/fMRI_POFA/061a.tif\n",
      "291.7982 POFA/fMRI_POFA/053a.tif\n",
      "296.8455 POFA/fMRI_POFA/105a.tif\n",
      "301.8296 POFA/fMRI_POFA/083a.tif\n",
      "306.8137 POFA/fMRI_POFA/006a.tif\n",
      "311.8294 POFA/fMRI_POFA/013a.tif\n",
      "316.8140 POFA/fMRI_POFA/033a.tif\n",
      "321.8008 POFA/fMRI_POFA/021a.tif\n",
      "326.7985 POFA/fMRI_POFA/072a.tif\n",
      "361.7673 POFA/fMRI_POFA/086a.tif\n",
      "366.7521 POFA/fMRI_POFA/075a.tif\n",
      "371.7521 POFA/fMRI_POFA/049a.tif\n",
      "376.7520 POFA/fMRI_POFA/008a.tif\n",
      "381.7522 POFA/fMRI_POFA/023a.tif\n",
      "386.7523 POFA/fMRI_POFA/036a.tif\n",
      "391.7677 POFA/fMRI_POFA/018a.tif\n",
      "396.7521 POFA/fMRI_POFA/053a.tif\n",
      "401.7524 POFA/fMRI_POFA/061a.tif\n",
      "406.7523 POFA/fMRI_POFA/080a.tif\n",
      "411.7525 POFA/fMRI_POFA/105a.tif\n",
      "416.7526 POFA/fMRI_POFA/069a.tif\n",
      "421.7679 POFA/fMRI_POFA/013a.tif\n",
      "426.7368 POFA/fMRI_POFA/021a.tif\n",
      "431.7527 POFA/fMRI_POFA/006a.tif\n",
      "436.7214 POFA/fMRI_POFA/033a.tif\n",
      "441.7527 POFA/fMRI_POFA/072a.tif\n",
      "446.7529 POFA/fMRI_POFA/083a.tif\n",
      "451.7682 POFA/fMRI_POFA/083c.tif\n",
      "456.7528 POFA/fMRI_POFA/021c.tif\n",
      "461.7687 POFA/fMRI_POFA/033c.tif\n",
      "466.7374 POFA/fMRI_POFA/006c.tif\n",
      "471.7529 POFA/fMRI_POFA/072c.tif\n",
      "476.7530 POFA/fMRI_POFA/013c.tif\n"
     ]
    }
   ],
   "source": [
    "def parse_psychopy_log(log_path):\n",
    "    with open(log_path) as log:\n",
    "        log = log.readlines()\n",
    "    for entry in log:\n",
    "        (timestamp, level, description) = entry.split(\" \t\")\n",
    "        if description.split()[1] == \"image\" and description.split()[0][:6] == \"image_\":\n",
    "            #print(description.split(\" \")[0][:5])\n",
    "            #print(description.split()[0][:6])\n",
    "            # a lil slow but it's readable\n",
    "            image = description.split()[3].replace(\"'\",\"\")\n",
    "            print(timestamp, image)\n",
    "\n",
    "\n",
    "#TODO Frame Info:\n",
    "# what is on the screen (image)\n",
    "# What input is happening at that frame (keypresses)\n",
    "# later: the timeslice of the fMRI\n",
    "# Integrate into view fMRI\n",
    "parse_psychopy_log(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is cool and all, but let's get a working volume in blender before anything else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize BOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [study](https://openneuro.org/datasets/ds003548/versions/1.0.1) the resting state fMRI is ten minutes long. Some chatbots suggested that the 4th index in the `ANTsImage` `Spacing` Tuple would be the time spacing, and correspond to the number of seconds each frame is taken at.\n",
    "\n",
    "To verify this, let's make sure that $\\frac{Slice Duration * Number Of Slices}{60}=10$\n",
    "\n",
    "Confusingly enough, Dimensions is the name in the return string when printed, while these values are accessed by `.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTsImage\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (64, 64, 35, 300)\n",
      "\t Spacing    : (4.0, 4.0, 4.0, 2.0)\n",
      "\t Origin     : (-127.953, 108.933, -74.8393, 0.0)\n",
      "\t Direction  : [ 1.  0.  0.  0.  0. -1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.]\n",
      "\n",
      "<class 'int'>\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "print(raw_rest_bold_img)\n",
    "minutes = (raw_rest_bold_img.spacing[3] * float(raw_rest_bold_img.shape[3])) / 60.0\n",
    "print(type(raw_rest_bold_img.shape[3]))\n",
    "print(minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good to me. Let's take this assumption that `.spacing[3]` will be the duration of the slices in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(events_tsv_path, 'r') as f:\n",
    "    events_tsv = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO display actual stimulus\n",
    "# TODO show all three spatial axes in one display\n",
    "# TODO rotate these displays\n",
    "# TODO once integrated add masking\n",
    "\n",
    "def explore_fMRI(ants_img: ants.core.ants_image.ANTsImage,\n",
    "                volume_override = \"NULL\",\n",
    "                 dim=\"x\", events_tsv=\"NULL\",\n",
    "                 cmap='nipy_spectral'):\n",
    "    if type(volume_override) == np.ndarray:\n",
    "        vol = volume_override\n",
    "    else:\n",
    "        vol = ants_img.numpy()\n",
    "    \n",
    "    def dim_to_indexed(dim, slice, frame):\n",
    "        match dim:\n",
    "            case \"x\":\n",
    "                return vol[slice,:,:,frame]\n",
    "            case \"y\":\n",
    "                return vol[:,slice,:,frame]\n",
    "            case \"z\":\n",
    "                return vol[:,:,slice,frame]\n",
    "\n",
    "    def plot(slice, frame):\n",
    "        second = float(frame * ants_img.spacing[3])\n",
    "        plt.figure()\n",
    "        plt.imshow(dim_to_indexed(dim, slice, frame), cmap=cmap)\n",
    "        plt.show()\n",
    "        present_event = \"No event file\"\n",
    "        if events_tsv != \"NULL\":\n",
    "            for event in events_tsv.split(\"\\n\"):\n",
    "                info = event.split(\"\t\")\n",
    "                if info[0].isdigit() and info[1].isdigit():\n",
    "                    if float(info[0]) <= second < float(info[0] + info[1]):\n",
    "                        present_event = info[2]\n",
    "            print(present_event)\n",
    "\n",
    "    frame_slider = (0, (vol.shape[3]-1))\n",
    "    match dim:\n",
    "        case \"x\":\n",
    "            interact(plot, slice=(0, vol.shape[0]-1), frame=frame_slider)\n",
    "        case \"y\":\n",
    "            interact(plot, slice=(0, vol.shape[1]-1), frame=frame_slider)\n",
    "        case \"z\":\n",
    "            interact(plot, slice=(0, vol.shape[2]-1), frame=frame_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f18a7802d9a4efca94b8e580b4442b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_fMRI(raw_stim_bold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract Baseline from Stimulus to Isolate the Activations\n",
    "\n",
    "I wrote this out when I didn't understand that the scrambled faces were the neutral stimulus. It is thus, probably deep scientifically problematic, but it might be useful as I dig deeper into the intricacies of experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_average_rest = np.mean(raw_rest_bold_img.numpy(), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e757d0d46de7423d8cae85287cbf295f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 35)\n"
     ]
    }
   ],
   "source": [
    "explore_3D_vol(temporal_average_rest)\n",
    "print(temporal_average_rest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_neutral_3D(experimental, neutral):\n",
    "    \"\"\"\n",
    "    Subtracts a 3D neutral from each time slice in a 4D experimental volume\n",
    "    For use in experiments which lack a block design and for which the \n",
    "    neutral stimulus has been derived from an averaged rest state\n",
    "    \"\"\"\n",
    "    result = np.empty_like(experimental)\n",
    "    for time_slice in range(experimental.shape[3]):\n",
    "        result[:,:,:,time_slice] = experimental[:,:,:, time_slice] - neutral\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_BOLD = subtract_neutral_3D(raw_stim_bold.numpy(), temporal_average_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(isolated_BOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdca041d96b40a885c4c94de4099038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_fMRI(raw_stim_bold, volume_override=isolated_BOLD, dim=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Change in Volume\n",
    "Here's another possible approach. Again, I do not know the scientific validity of this. Here we'll take each frame and measure the difference in the bold response from the previous frame.\n",
    "\n",
    "Grabbing the absolute value between the current and previous timestamp creates the most convenient isolation for our visualization tool. However, this does not mean that it is the most scientifically valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_BOLD_movement(bold_vol, baseline_vol):\n",
    "    \"\"\"\n",
    "    Each frame shows the difference between it and the previous frame. First frame is initialized at zero. \n",
    "    \"\"\"\n",
    "    result = np.empty_like(bold_vol)\n",
    "    for time_slice in range(1, bold_vol.shape[3]):\n",
    "        result[:,:,:,time_slice] = np.absolute(bold_vol[:,:,:,time_slice] - bold_vol[:,:,:,time_slice - 1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_movement = measure_BOLD_movement(raw_stim_bold.numpy(), temporal_average_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using volume override\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488bf16f4a62413b9ea73eb04fa9c800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_fMRI(raw_stim_bold, volume_override=bold_movement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract Neutral from Stimulus\n",
    "\n",
    "The previous experiments have been illuminating, but let's do this thing for real:\n",
    "\n",
    "Subtract the scrambled stimulus from the experimental stimulus to view the activations.\n",
    "\n",
    "For simplicities sake: we'll use the first scrambled image as our neutral. This way we also see the difference between two scrambled images!\n",
    "\n",
    "There's all sorts of experimental design and analysis stuff in here that I'm curious about. All the more reason to build this into a nice, node-based GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onset\tduration\ttrial_type\n",
      "0\t30\tblank\n",
      "30\t30\tscrambled\n",
      "60\t30\thappy\n",
      "90\t30\tsad\n",
      "120\t30\tangry\n",
      "150\t30\tneutral\n",
      "180\t30\thappy\n",
      "210\t30\tsad\n",
      "240\t30\tangry\n",
      "270\t30\tneutral\n",
      "300\t30\tscrambled\n",
      "330\t30\tblank\n",
      "360\t10\tend\n",
      "\n",
      "\n",
      "fourth dimension of ANTs image: 2.0\n"
     ]
    }
   ],
   "source": [
    "print(events_tsv)\n",
    "print(f\"fourth dimension of ANTs image: {raw_stim_bold.spacing[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the worlds most elegant programming choice, I know, but let's just hard code in the fact that we know the `scrambled` stimulus takes place from second `30` until second `60`. We also know that the timescale is `2.0`, so every frame of the `BOLD` image will equal seconds (but we can grab that from the `ANTsImage`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 35, 185)\n"
     ]
    }
   ],
   "source": [
    "def segment_fMRI(ants_bold: ants.core.ants_image.ANTsImage, io: tuple):\n",
    "    i, o = io\n",
    "    fmri_vol = ants_bold.numpy()\n",
    "    print(fmri_vol.shape)\n",
    "    return fmri_vol[:,:,:,[i,o]]\n",
    "neutral = segment_fMRI(raw_stim_bold, (30, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_fMRI(raw_stim_bold, vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register and Mask BOLD and Anat\n",
    "\n",
    "We're going to go wit the `bold_movement` volume. Not sure if it's the best, but it's the most convenient to visualize in this context. Or maybe we mask/register the bold first? Is that the best way of going about it?\n",
    "\n",
    "We're also going to need to account for motion correction and size differences between anat and bold. Oh boy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mni_template = ants.image_read(mni_anat_filepath)\n",
    "mni_mask = ants.image_read(mni_mask_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af004a6af924de79b0477772b5bf70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Slicing methodology check\n",
    "sliced_bold = ants.from_numpy(raw_stim_bold.numpy()[:,:,:,50])\n",
    "sliced_bold02 = ants.from_numpy(raw_stim_bold.numpy()[:,:,:,100])\n",
    "explore_3D_vol(sliced_bold.numpy())\n",
    "print(sliced_bold is sliced_bold02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_masking(bold_img, template, mask, dilate=True):\n",
    "    indent = \"        ➡️\"\n",
    "    print(\"Masking bold. This might take a while...\")\n",
    "    result = np.empty_like(bold_img.numpy())\n",
    "\n",
    "\n",
    "    for time_slice in range(bold_img.numpy().shape[3]):\n",
    "        bold_slice = ants.from_numpy(bold_img.numpy()[:,:,:,time_slice])\n",
    "\n",
    "        print(f\"Masking for time slice {time_slice} out of {bold_img.numpy().shape[3]}\")\n",
    "        print(f\"{indent}creating template\")\n",
    "        template_warp_to_bold_anat = ants.registration(\n",
    "            fixed=bold_slice,\n",
    "            moving=template, \n",
    "            type_of_transform='SyN',\n",
    "            verbose=False\n",
    "            )\n",
    "        \n",
    "        print(f\"{indent}Registering template image\")\n",
    "\n",
    "        print(f\"{indent}Creating brain mask\")\n",
    "        brain_mask = ants.apply_transforms(\n",
    "            fixed=template_warp_to_bold_anat['warpedmovout'],\n",
    "            moving=mask,\n",
    "            transformlist=template_warp_to_bold_anat['fwdtransforms'],\n",
    "            interpolator='nearestNeighbor',\n",
    "            verbose=False\n",
    "            )\n",
    "        if dilate:\n",
    "            print(f\"{indent}Dilating brian mask\")\n",
    "            brain_mask = ants.morphology(brain_mask, radius=4, operation='dilate', mtype='binary')\n",
    "        print(f\"{indent}Applying brain mask and adding to final result\")\n",
    "        result[:,:,:,time_slice] = ants.mask_image(bold_slice, brain_mask).numpy()\n",
    "    return ants.from_numpy(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masked_bold = bold_masking(raw_stim_bold, mni_template, mni_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore_4D_vol(masked_bold)\n",
    "# explore_4D_vol(raw_stim_bold.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it looks like this is the exact same thing\n",
    "\n",
    "looking at the mri, though, do we even need to mask the brain? Can we just threshold out the purple stuff?\n",
    "\n",
    "The following is a very lovely function but it wasn't very smart of you to write it. If you look at the BOLD respoonse in this dataset you can see that we can isolate the brain just with grid-specific threshholding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register BOLD to T1\n",
    "\n",
    "Like the above function, this will register the BOLD function to the anat. A more typical analysis pipeline might register to MNI space, as above, but we don't really care about voxel-specific cross-study analysis; we want a subject-anatomy specific visualization.\n",
    "\n",
    "We're reusing much of the logic above, and will have to write custom viewer functions to verify our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = ants.image_read(\"/Users/joachimpfefferkorn/Downloads/sub-01122021301_task-arousal_bold.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_fMRI(new_dataset.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_dataset)\n",
    "print(raw_stim_bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
