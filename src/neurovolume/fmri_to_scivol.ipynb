{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering fMRI Data\n",
    "Both anatomical and functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_viewer_functions import *\n",
    "from functions import *\n",
    "from scivol import *\n",
    "import numpy as np\n",
    "import json\n",
    "import ants\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root: /Users/joachimpfefferkorn/repos/neurovolume\n"
     ]
    }
   ],
   "source": [
    "proj_root = parent_directory()\n",
    "print(f\"project root: {proj_root}\")\n",
    "t1_input_filepath = os.path.join(proj_root, \"media/sub-01/anat/sub-01_T1w.nii.gz\")\n",
    "bold_stim_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-emotionalfaces_run-1_bold.nii.gz\")\n",
    "bold_rest_filepath = os.path.join(proj_root, \"media/sub-01/func/sub-01_task-rest_bold.nii.gz\")\n",
    "mni_anat_filepath =  os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a.nii\")\n",
    "mni_mask_filepath = os.path.join(proj_root, \"templates/mni_icbm152_t1_tal_nlin_sym_09a_mask.nii\")\n",
    "events_tsv_path = os.path.join(proj_root, \"media/sub-01/func/task-emotionalfaces_run-1_events.tsv\")\n",
    "stimulus_image_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/POFA/fMRI_POFA\"\n",
    "log_path = \"/Users/joachimpfefferkorn/repos/emotional-faces-psychopy-task-main/emofaces/data/01-subject_emofaces1_2019_Aug_14_1903.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given how specific this notebook is getting to this study, maybe rename the stimulus files to something like `emofaces_run_1_bold`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_t1_img = ants.image_read(t1_input_filepath)\n",
    "raw_stim_bold = ants.image_read(bold_stim_filepath)\n",
    "raw_rest_bold_img = ants.image_read(bold_rest_filepath)\n",
    "mni_img = ants.image_read(mni_anat_filepath)\n",
    "mni_mask_img = ants.image_read(mni_mask_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Stimulus Data\n",
    "I am somewhat mystified by the `csv` formatting for from PsychoPy. I am going to start by parsing it from the log file. This is perhaps not the most robust way to do it, but it will be quicker than reading through and testing all the pyschopy documentation. It also provides a ton of metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181.9064 POFA/fMRI_POFA/021c.tif\n",
      "186.8437 POFA/fMRI_POFA/072c.tif\n",
      "191.8438 POFA/fMRI_POFA/013c.tif\n",
      "196.8440 POFA/fMRI_POFA/033c.tif\n",
      "201.8439 POFA/fMRI_POFA/083c.tif\n",
      "206.8440 POFA/fMRI_POFA/006c.tif\n",
      "241.8758 POFA/fMRI_POFA/036a.tif\n",
      "246.8443 POFA/fMRI_POFA/008a.tif\n",
      "251.8445 POFA/fMRI_POFA/075a.tif\n",
      "256.8601 POFA/fMRI_POFA/049a.tif\n",
      "261.8443 POFA/fMRI_POFA/023a.tif\n",
      "266.8448 POFA/fMRI_POFA/086a.tif\n",
      "271.8603 POFA/fMRI_POFA/069a.tif\n",
      "276.8136 POFA/fMRI_POFA/080a.tif\n",
      "283.4543 POFA/fMRI_POFA/018a.tif\n",
      "287.1106 POFA/fMRI_POFA/061a.tif\n",
      "291.7982 POFA/fMRI_POFA/053a.tif\n",
      "296.8455 POFA/fMRI_POFA/105a.tif\n",
      "301.8296 POFA/fMRI_POFA/083a.tif\n",
      "306.8137 POFA/fMRI_POFA/006a.tif\n",
      "311.8294 POFA/fMRI_POFA/013a.tif\n",
      "316.8140 POFA/fMRI_POFA/033a.tif\n",
      "321.8008 POFA/fMRI_POFA/021a.tif\n",
      "326.7985 POFA/fMRI_POFA/072a.tif\n",
      "361.7673 POFA/fMRI_POFA/086a.tif\n",
      "366.7521 POFA/fMRI_POFA/075a.tif\n",
      "371.7521 POFA/fMRI_POFA/049a.tif\n",
      "376.7520 POFA/fMRI_POFA/008a.tif\n",
      "381.7522 POFA/fMRI_POFA/023a.tif\n",
      "386.7523 POFA/fMRI_POFA/036a.tif\n",
      "391.7677 POFA/fMRI_POFA/018a.tif\n",
      "396.7521 POFA/fMRI_POFA/053a.tif\n",
      "401.7524 POFA/fMRI_POFA/061a.tif\n",
      "406.7523 POFA/fMRI_POFA/080a.tif\n",
      "411.7525 POFA/fMRI_POFA/105a.tif\n",
      "416.7526 POFA/fMRI_POFA/069a.tif\n",
      "421.7679 POFA/fMRI_POFA/013a.tif\n",
      "426.7368 POFA/fMRI_POFA/021a.tif\n",
      "431.7527 POFA/fMRI_POFA/006a.tif\n",
      "436.7214 POFA/fMRI_POFA/033a.tif\n",
      "441.7527 POFA/fMRI_POFA/072a.tif\n",
      "446.7529 POFA/fMRI_POFA/083a.tif\n",
      "451.7682 POFA/fMRI_POFA/083c.tif\n",
      "456.7528 POFA/fMRI_POFA/021c.tif\n",
      "461.7687 POFA/fMRI_POFA/033c.tif\n",
      "466.7374 POFA/fMRI_POFA/006c.tif\n",
      "471.7529 POFA/fMRI_POFA/072c.tif\n",
      "476.7530 POFA/fMRI_POFA/013c.tif\n"
     ]
    }
   ],
   "source": [
    "def parse_psychopy_log(log_path):\n",
    "    with open(log_path) as log:\n",
    "        log = log.readlines()\n",
    "    for entry in log:\n",
    "        (timestamp, level, description) = entry.split(\" \t\")\n",
    "        if description.split()[1] == \"image\" and description.split()[0][:6] == \"image_\":\n",
    "            #print(description.split(\" \")[0][:5])\n",
    "            #print(description.split()[0][:6])\n",
    "            # a lil slow but it's readable\n",
    "            image = description.split()[3].replace(\"'\",\"\")\n",
    "            print(timestamp, image)\n",
    "\n",
    "\n",
    "#TODO Frame Info:\n",
    "# what is on the screen (image)\n",
    "# What input is happening at that frame (keypresses)\n",
    "# later: the timeslice of the fMRI\n",
    "# Integrate into view fMRI\n",
    "parse_psychopy_log(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is cool and all, but **let's get a working volume in blender before anything else**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize BOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [study](https://openneuro.org/datasets/ds003548/versions/1.0.1) the resting state fMRI is ten minutes long. Some chatbots suggested that the 4th index in the `ANTsImage` `Spacing` Tuple would be the time spacing, and correspond to the number of seconds each frame is taken at.\n",
    "\n",
    "To verify this, let's make sure that $\\frac{Slice Duration * Number Of Slices}{60}=10$\n",
    "\n",
    "Confusingly enough, Dimensions is the name in the return string when printed, while these values are accessed by `.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTsImage\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (64, 64, 35, 300)\n",
      "\t Spacing    : (4.0, 4.0, 4.0, 2.0)\n",
      "\t Origin     : (-127.953, 108.933, -74.8393, 0.0)\n",
      "\t Direction  : [ 1.  0.  0.  0.  0. -1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.]\n",
      "\n",
      "<class 'int'>\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "print(raw_rest_bold_img)\n",
    "minutes = (raw_rest_bold_img.spacing[3] * float(raw_rest_bold_img.shape[3])) / 60.0\n",
    "print(type(raw_rest_bold_img.shape[3]))\n",
    "print(minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good to me. Let's take this assumption that `.spacing[3]` will be the duration of the slices in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(events_tsv_path, 'r') as f:\n",
    "    events_tsv = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO display actual stimulus\n",
    "# TODO interact with seconds, not frames (gives a nice sanity check for BOLD responses)\n",
    "# TODO show all three spatial axes in one display\n",
    "# TODO rotate these displays\n",
    "# TODO once integrated add masking\n",
    "\n",
    "def explore_fMRI(ants_img: ants.core.ants_image.ANTsImage,\n",
    "                volume_override = \"NULL\",\n",
    "                 dim=\"x\", events_tsv=\"NULL\",\n",
    "                 cmap='nipy_spectral'):\n",
    "    if type(volume_override) == np.ndarray:\n",
    "        vol = volume_override\n",
    "    else:\n",
    "        vol = ants_img.numpy()\n",
    "    \n",
    "    def dim_to_indexed(dim, slice, frame):\n",
    "        match dim:\n",
    "            case \"x\":\n",
    "                return vol[slice,:,:,frame]\n",
    "            case \"y\":\n",
    "                return vol[:,slice,:,frame]\n",
    "            case \"z\":\n",
    "                return vol[:,:,slice,frame]\n",
    "\n",
    "    def plot(slice, frame):\n",
    "        second = float(frame * ants_img.spacing[3])\n",
    "        plt.figure()\n",
    "        plt.imshow(dim_to_indexed(dim, slice, frame), cmap=cmap)\n",
    "        plt.show()\n",
    "        present_event = \"No event file\"\n",
    "        if events_tsv != \"NULL\":\n",
    "            for event in events_tsv.split(\"\\n\"):\n",
    "                info = event.split(\"\t\")\n",
    "                if info[0].isdigit() and info[1].isdigit():\n",
    "                    if float(info[0]) <= second < float(info[0] + info[1]):\n",
    "                        present_event = info[2]\n",
    "            print(present_event)\n",
    "\n",
    "    frame_slider = (0, (vol.shape[3]-1))\n",
    "    match dim:\n",
    "        case \"x\":\n",
    "            interact(plot, slice=(0, vol.shape[0]-1), frame=frame_slider)\n",
    "        case \"y\":\n",
    "            interact(plot, slice=(0, vol.shape[1]-1), frame=frame_slider)\n",
    "        case \"z\":\n",
    "            interact(plot, slice=(0, vol.shape[2]-1), frame=frame_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore_fMRI(raw_stim_bold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract Baseline from Stimulus to Isolate the Activations\n",
    "\n",
    "I wrote this out when I didn't understand that the scrambled faces were the neutral stimulus. It is thus, probably deep scientifically problematic, but it might be useful as I dig deeper into the intricacies of experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_average_rest = np.mean(raw_rest_bold_img.numpy(), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c91e8032eb468499c237b3bcb283ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 35)\n"
     ]
    }
   ],
   "source": [
    "explore_3D_vol(temporal_average_rest)\n",
    "print(temporal_average_rest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_neutral_3D(experimental, neutral):\n",
    "    \"\"\"\n",
    "    Subtracts a 3D neutral from each time slice in a 4D experimental volume\n",
    "    For use in experiments which lack a block design and for which the \n",
    "    neutral stimulus has been derived from an averaged rest state\n",
    "    \"\"\"\n",
    "    result = np.empty_like(experimental)\n",
    "    for time_slice in range(experimental.shape[3]):\n",
    "        result[:,:,:,time_slice] = experimental[:,:,:, time_slice] - neutral\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_BOLD = subtract_neutral_3D(raw_stim_bold.numpy(), temporal_average_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(isolated_BOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe4573b780c4ce59266df6ffc6c9af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_fMRI(raw_stim_bold, volume_override=isolated_BOLD, dim=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Change in Volume\n",
    "Here's another possible approach. Again, I do not know the scientific validity of this. Here we'll take each frame and measure the difference in the bold response from the previous frame.\n",
    "\n",
    "Grabbing the absolute value between the current and previous timestamp creates the most convenient isolation for our visualization tool. However, this does not mean that it is the most scientifically valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_BOLD_movement(bold_vol, baseline_vol):\n",
    "    \"\"\"\n",
    "    Each frame shows the difference between it and the previous frame. First frame is initialized at zero. \n",
    "    \"\"\"\n",
    "    result = np.empty_like(bold_vol)\n",
    "    for time_slice in range(1, bold_vol.shape[3]):\n",
    "        result[:,:,:,time_slice] = np.absolute(bold_vol[:,:,:,time_slice] - bold_vol[:,:,:,time_slice - 1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_movement = measure_BOLD_movement(raw_stim_bold.numpy(), temporal_average_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ec76e8388d4a2ea488be77ed8a9d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_fMRI(raw_stim_bold, volume_override=bold_movement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract Neutral from Stimulus\n",
    "\n",
    "The previous experiments have been illuminating, but let's do this thing for real:\n",
    "\n",
    "Subtract the scrambled stimulus from the experimental stimulus to view the activations.\n",
    "\n",
    "For simplicities sake: we'll use the first scrambled image as our neutral. This way we also see the difference between two scrambled images!\n",
    "\n",
    "Hopefully the 30 second durations give us enough time to show a good hemodynamic response. The more I dig into this the more questions I have. Is it best practice to average these two neutral stimuli?\n",
    "\n",
    "There's all sorts of experimental design and analysis stuff in here that I'm curious about. **All the more reason to build this into a nice, node-based GUI!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onset\tduration\ttrial_type\n",
      "0\t30\tblank\n",
      "30\t30\tscrambled\n",
      "60\t30\thappy\n",
      "90\t30\tsad\n",
      "120\t30\tangry\n",
      "150\t30\tneutral\n",
      "180\t30\thappy\n",
      "210\t30\tsad\n",
      "240\t30\tangry\n",
      "270\t30\tneutral\n",
      "300\t30\tscrambled\n",
      "330\t30\tblank\n",
      "360\t10\tend\n",
      "\n",
      "\n",
      "fourth dimension of ANTs image: 2.0\n"
     ]
    }
   ],
   "source": [
    "print(events_tsv)\n",
    "print(f\"fourth dimension of ANTs image: {raw_stim_bold.spacing[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the worlds most elegant programming choice, I know, but let's just hard code in the fact that we know the `scrambled` stimulus takes place from second `30` until second `60`. We also know that the timescale is `2.0`, so every frame of the `BOLD` image will equal seconds (but we can grab that from the `ANTsImage`)\n",
    "\n",
    "A **more accurate approach would be to use the log file from the experiment!** The more I dig into this, the more I'm warming up to parsing this log file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 35, 185)\n",
      "(64, 64, 35, 15)\n"
     ]
    }
   ],
   "source": [
    "# A little tired today, so double check this (relatively simple) function later\n",
    "def segment_fMRI(ants_bold: ants.core.ants_image.ANTsImage, i_second, o_second):\n",
    "    i_frame = int(i_second / ants_bold.spacing[3]) #second divided by the frame rate\n",
    "    o_frame = int(o_second / ants_bold.spacing[3]) #second divided by the frame rate\n",
    "    fmri_vol = ants_bold.numpy()\n",
    "    return fmri_vol[:,:,:,i_frame:o_frame]\n",
    "\n",
    "print(raw_stim_bold.shape)\n",
    "control = segment_fMRI(raw_stim_bold, 30, 60)\n",
    "print(control.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore_fMRI(raw_stim_bold, volume_override=control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So eventually we'll have all this come from a fully fledged log (or at the least csv) parsing thing, but for now we'll just do it in this quick, hack-y fashion.\n",
    "\n",
    "Likewise, it will be good to build a custom fMRI object that includes the stimulus and action data encoded along the temporal dimension. Possibly integrate into scivol. **Scivol really should include \"events\" such as these!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_neutral_4D(experimental, control):\n",
    "    # So this is just assuming that everything is on a 30 second block which, starting\n",
    "    # at zero.\n",
    "    # Another reason to do this from the log, honestly.\n",
    "    # These should be triggered by \"events\" in the scivol, tbh!\n",
    "\n",
    "    result = np.empty_like(experimental)\n",
    "    control_frame = -1\n",
    "\n",
    "    for experimental_frame in range(experimental.shape[3]):\n",
    "        if control_frame >= control.shape[3] - 1:\n",
    "            control_frame = 0\n",
    "        else:\n",
    "            control_frame += 1\n",
    "        result[:,:,:,experimental_frame] = experimental[:,:,:,experimental_frame] - control[:,:,:, control_frame]\n",
    "\n",
    "        #print(experimental_frame, control_frame)\n",
    "diffed_4D = subtract_neutral_4D(raw_stim_bold.numpy(), control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528c595e376b4c6f9a5b8ff356b5a563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb15f00aeeac483aac9824e5f4ea9dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), IntSlider(value=92, description='frame…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_fMRI(raw_stim_bold, volume_override=diffed_4D)\n",
    "explore_fMRI(raw_stim_bold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aaaaaaand these results look identical?\n",
    "\n",
    "\n",
    "**Let's put a pin in this for now and work on the registration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register and Mask BOLD and Anat\n",
    "\n",
    "We're going to go wit the `bold_movement` volume for now. Not sure if it's the best, but it's the most convenient to visualize in this context. Or maybe we mask/register the bold first? Is that the best way of going about it?\n",
    "\n",
    "We're also going to need to account for motion correction and size differences between anat and bold. Oh boy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mni_template = ants.image_read(mni_anat_filepath)\n",
    "mni_mask = ants.image_read(mni_mask_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9e88faec9d4e5681fbb646e2d6e0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=31, description='slice', max=63), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Slicing methodology check\n",
    "sliced_bold = ants.from_numpy(raw_stim_bold.numpy()[:,:,:,50])\n",
    "sliced_bold02 = ants.from_numpy(raw_stim_bold.numpy()[:,:,:,100])\n",
    "explore_3D_vol(sliced_bold.numpy())\n",
    "print(sliced_bold is sliced_bold02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_masking(bold_img, template, mask, dilate=True):\n",
    "    indent = \"        ➡️\"\n",
    "    print(\"Masking bold. This might take a while...\")\n",
    "    result = np.empty_like(bold_img.numpy())\n",
    "\n",
    "\n",
    "    for time_slice in range(bold_img.numpy().shape[3]):\n",
    "        bold_slice = ants.from_numpy(bold_img.numpy()[:,:,:,time_slice])\n",
    "\n",
    "        print(f\"Masking for time slice {time_slice} out of {bold_img.numpy().shape[3]}\")\n",
    "        print(f\"{indent}creating template\")\n",
    "        template_warp_to_bold_anat = ants.registration(\n",
    "            fixed=bold_slice,\n",
    "            moving=template, \n",
    "            type_of_transform='SyN',\n",
    "            verbose=False\n",
    "            )\n",
    "        \n",
    "        print(f\"{indent}Registering template image\")\n",
    "\n",
    "        print(f\"{indent}Creating brain mask\")\n",
    "        brain_mask = ants.apply_transforms(\n",
    "            fixed=template_warp_to_bold_anat['warpedmovout'],\n",
    "            moving=mask,\n",
    "            transformlist=template_warp_to_bold_anat['fwdtransforms'],\n",
    "            interpolator='nearestNeighbor',\n",
    "            verbose=False\n",
    "            )\n",
    "        if dilate:\n",
    "            print(f\"{indent}Dilating brian mask\")\n",
    "            brain_mask = ants.morphology(brain_mask, radius=4, operation='dilate', mtype='binary')\n",
    "        print(f\"{indent}Applying brain mask and adding to final result\")\n",
    "        result[:,:,:,time_slice] = ants.mask_image(bold_slice, brain_mask).numpy()\n",
    "    return ants.from_numpy(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masked_bold = bold_masking(raw_stim_bold, mni_template, mni_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore_4D_vol(masked_bold)\n",
    "# explore_4D_vol(raw_stim_bold.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it looks like this is the exact same thing\n",
    "\n",
    "looking at the mri, though, do we even need to mask the brain? Can we just threshold out the purple stuff?\n",
    "\n",
    "The following is a very lovely function but it wasn't very smart of you to write it. If you look at the BOLD respoonse in this dataset you can see that we can isolate the brain just with grid-specific threshholding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register BOLD to T1\n",
    "\n",
    "Like the above function, this will register the BOLD function to the anat. A more typical analysis pipeline might register to MNI space, as above, but we don't really care about voxel-specific cross-study analysis; we want a subject-anatomy specific visualization.\n",
    "\n",
    "We're reusing much of the logic above, and will have to write custom viewer functions to verify our work\n",
    "\n",
    "I think this is a good order of operations:\n",
    "1. Register each frame of the BOLD image to the anatomy T1 image\n",
    "2. Perform method of subtraction to isolate brain activity on/using this newly registered brain activity\n",
    "3. Write these as separate scivol grids (you'll have to do grid-specific thresholding now, tbh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_bold_to_anat(bold_img, anat_img):\n",
    "    print(\"Registering BOLD img to static anatomy.\\nThis may take a while\")\n",
    "    result_np = np.empty((anat_img.shape[0], anat_img.shape[1], anat_img.shape[2], bold_img.shape[3]))\n",
    "    print(f\"Final array dimensions: {result_np.shape}\")\n",
    "\n",
    "    for frame in range(bold_img.numpy().shape[3]):\n",
    "        print(f\"Registering BOLD frame {frame}/{bold_img.numpy().shape[3]}\")\n",
    "        bold_frame_img = ants.from_numpy(bold_img.numpy()[:,:,:,frame])\n",
    "        try:\n",
    "            registered_frame = ants.registration(\n",
    "                fixed=anat_img,\n",
    "                moving=bold_frame_img,\n",
    "                type_of_transform='SyNBold',\n",
    "                verbose=False\n",
    "            )\n",
    "            return registered_frame['warpedmovout'].numpy() #quick lil check\n",
    "        except Exception as e:\n",
    "            print(f\"Error registering frame {frame}:\\n{e}\")\n",
    "            return e\n",
    "        registered_frame_array = registered_frame['warpedmovout'].numpy()\n",
    "        print(f\"Adding registered frame to results\\nRegistered Frame Array: {registered_frame_array.shape}\\nResults: {result_np[:,:,:,frame].shape}\")\n",
    "#        result_np[:,:,:,frame] = registered_frame_array #THIS IS WHERE IT SEEMS TO CRASH\n",
    "    # print(\"Converting numpy array to ANTS img\")\n",
    "    # results_img = ants.from_numpy(result_np)\n",
    "    # return results_img\n",
    "    #return(result_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering BOLD img to static anatomy.\n",
      "This may take a while\n",
      "Final array dimensions: (512, 512, 296, 185)\n",
      "Registering BOLD frame 0/185\n"
     ]
    }
   ],
   "source": [
    "registered_bold = register_bold_to_anat(raw_stim_bold, raw_t1_img)\n",
    "#okay so it does register, but crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_3D_vol(registered_bold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
